{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ChpAIG52t_w0"
   },
   "source": [
    "#author: Jedrzej Chmiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "GXNAt4ojA9pK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345626478,
     "user_tz": -120,
     "elapsed": 7,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFwFtxvSlrxX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345663088,
     "user_tz": -120,
     "elapsed": 36616,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "27503175-ecb3-45e5-c43a-d004153029b5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting nltk==3.7\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.5 MB 30.0 MB/s \n",
      "\u001B[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001B[K     |████████████████████████████████| 749 kB 59.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.7) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.7) (4.64.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.7) (7.1.2)\n",
      "Installing collected packages: regex, nltk\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2019.12.20\n",
      "    Uninstalling regex-2019.12.20:\n",
      "      Successfully uninstalled regex-2019.12.20\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.7 regex-2022.4.24\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.7"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://12jerek34jeremi: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@github.com/12jerek34jeremi/harry_potter.git#subdirectory=hpcw"
   ],
   "metadata": {
    "id": "N_Ww7L4yBVKR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345663149,
     "user_tz": -120,
     "elapsed": 137,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "bffbc27e-21fe-4e47-b201-23aa5f219f01"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://12jerek34jeremi:****@github.com/12jerek34jeremi/harry_potter.git#subdirectory=hpcw\n",
      "  Cloning https://12jerek34jeremi:****@github.com/12jerek34jeremi/harry_potter.git to /tmp/pip-req-build-gstu493r\n",
      "  Running command git clone -q 'https://12jerek34jeremi:****@github.com/12jerek34jeremi/harry_potter.git' /tmp/pip-req-build-gstu493r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from hpcw==0.1.0) (1.11.0+cu113)\n",
      "Requirement already satisfied: nltk==3.7 in /usr/local/lib/python3.7/dist-packages (from hpcw==0.1.0) (3.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.7->hpcw==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk==3.7->hpcw==0.1.0) (2022.4.24)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.7->hpcw==0.1.0) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.7->hpcw==0.1.0) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->hpcw==0.1.0) (4.2.0)\n",
      "Building wheels for collected packages: hpcw\n",
      "  Building wheel for hpcw (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for hpcw: filename=hpcw-0.1.0-py3-none-any.whl size=14064 sha256=4b1ae3573ec1a3dc84b5d59d8dd6064506612c05d1ee6dec407fdd562d6694fb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-axyh6fu7/wheels/db/3b/ad/7fbde81314b74a6b232925f5b648d728718062aa6e4ebaf0ca\n",
      "Successfully built hpcw\n",
      "Installing collected packages: hpcw\n",
      "Successfully installed hpcw-0.1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python --version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwZz-qLqTBv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345663150,
     "user_tz": -120,
     "elapsed": 125,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "bb971ddf-0501-45a9-d99f-44c74b1ccba2"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python 3.7.13\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "id": "1UrRDgIbt_w3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652346329007,
     "user_tz": -120,
     "elapsed": 3715,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from hpcw.datasets.words_batch_dataset import WordsBatchDataset\n",
    "from hpcw.datasets.one_item_dataset import OneItemDataset\n",
    "from hpcw.corpus import Corpus\n",
    "from hpcw.models.embedding import Embedding\n",
    "from hpcw.models.words_batch import WordsBatch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from hpcw.utils import count_distance\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vXXPsWOOjh8l",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345663152,
     "user_tz": -120,
     "elapsed": 122,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "850f7392-dd1f-4726-b7d5-c085bc7e748a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KqLBn7gtuMyw",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345663153,
     "user_tz": -120,
     "elapsed": 120,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "bffe3018-2abb-47d2-a4f3-1fba54c02def"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'cuda'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Bnej8iGMt_w4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345699360,
     "user_tz": -120,
     "elapsed": 36324,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "bf7028a4-af26-48dc-8c08-694f7e3949c2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WuhW91W6vMRp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345699360,
     "user_tz": -120,
     "elapsed": 8,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "MAIN_DATA_DIR = 'drive/MyDrive/data_harry_potter'\n",
    "# MAIN_DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ucAT7QTAt_w4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345701222,
     "user_tz": -120,
     "elapsed": 1869,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "one_item_dataset = OneItemDataset(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0svU9iY0t_w4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745036,
     "user_tz": -120,
     "elapsed": 43819,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "words_batch_datasets = [WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_'+str(i)+'_prepared.txt', corpus.dictionary, sequence_length=6)\n",
    "                        for i in range(1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def test_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size=2048) -> float:\n",
    "    \"\"\"\n",
    "Tests passed word batch model using all datasets and returns MSE\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of WordsBatch class to be tested.\n",
    "    datasets:\n",
    "        List of onjects of WordsBatchDataset class. Datasets on which model will be tested.\n",
    "    \"\"\"\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "      loss_function = nn.MSELoss(reduction='sum')\n",
    "      for dataset in datasets:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        for X, y in loader:\n",
    "          X = X.to(DEVICE)\n",
    "          y = y.to(DEVICE)\n",
    "          y = model.embedding.to_dense(y)\n",
    "          pred = model(X)\n",
    "          loss = loss_function(pred, y)\n",
    "          mse +=  loss.item()\n",
    "    \n",
    "    total_length = sum([len(dataset) for dataset in datasets])\n",
    "    mse = mse / total_length\n",
    "    return mse"
   ],
   "metadata": {
    "id": "t_N5C7eOmLju",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745037,
     "user_tz": -120,
     "elapsed": 46,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qjVCsVrit_w5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745038,
     "user_tz": -120,
     "elapsed": 22,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "def train_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None, results:Dict[str, int or float] = None,\n",
    "                    start_epoch:int = 0):\n",
    "    \"\"\"\n",
    "This function trains words_batch model. It uses MSE loss function to do this.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of WordsBatch class.\n",
    "    datasets:\n",
    "        List of all datasets on which model will be trained. First function uses all inputs from first dataset,\n",
    "        then all input from second dataset, ..., then all outputs from last dataset. This is the end of first epoch.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of MSEs before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.MSELoss()\n",
    "    loaders = [DataLoader(dataset, batch_size, shuffle=True) for dataset in datasets]\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "        with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "        with open(saves_dir+'/results.txt', 'wt') as file:\n",
    "            for key, item in results.items():\n",
    "                file.write(key + ': ' + str(item) + '\\n')\n",
    "\n",
    "    end_epoch = start_epoch+epochs\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        for i,loader in enumerate(loaders):\n",
    "            print(f\"Epoch {epoch}/{end_epoch}, dataset {i+1}/7\")\n",
    "            for X, y in tqdm(loader, desc=\"batch: \"):\n",
    "                X = X.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    y = model.embedding.to_dense(y.to(DEVICE))\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if saves_dir is not None:\n",
    "            model.save(saves_dir+'/'+f\"words_batch_epoch_{epoch}.pth\")\n",
    "            if results is not None:\n",
    "                mse = test_words_batch(model, datasets)\n",
    "                print(f\"MSE is: {mse}\")\n",
    "                results[f'mse_after_epoch_{epoch}'] = mse\n",
    "                with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "                with open(saves_dir+'/results.txt', 'at') as file:\n",
    "                    file.write(f'mse_after_epoch_{epoch} : {mse}\\n')\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HVePgaVIt_w6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745039,
     "user_tz": -120,
     "elapsed": 22,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "def train_encoding(model: Embedding, dataset: OneItemDataset,  batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None, results:Dict[str, int or float] = None,\n",
    "                    start_epoch:int = 0):\n",
    "    \"\"\"\n",
    "This function trains encoding part of Embedding class object. It uses CrossEntropyLoss.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of Embedding class.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to train.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of acurate factor before each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    acurate = 0\n",
    "    acurates = []\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    if saves_dir is not None:\n",
    "            if not os.path.exists(saves_dir):\n",
    "                os.makedirs(saves_dir)\n",
    "            with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                        pickle.dump(results, file)\n",
    "            with open(saves_dir+'/results.txt', 'wt') as file:\n",
    "                for key, item in results.items():\n",
    "                    file.write(key + ': ' + str(item) + '\\n')\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch+epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        for y in tqdm(loader, desc=\"batch: \"):\n",
    "            with torch.no_grad():\n",
    "                X = model.to_dense(y)\n",
    "            pred = model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if saves_dir is not None:\n",
    "            model.save(saves_dir+'/'+f\"embedding_epoch_{epoch}.pth\")\n",
    "            if results is not None:\n",
    "                acurate = test_words_batch(model, dataset)\n",
    "                print(f\"acurate is: {acurate}\")\n",
    "                results[f'acurate_after_epoch_{epoch}'] = acurate\n",
    "                with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "                with open(saves_dir+'/results.txt', 'at') as file:\n",
    "                    file.write(f'acurate_after_epoch_{epoch} : {acurate}\\n')\n",
    "                model.train()\n",
    "\n",
    "\n",
    "        model.save(saves_dir+'/'+f\"embedding_epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M6LdFTmnt_w6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745039,
     "user_tz": -120,
     "elapsed": 21,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "def test_encoding(model, dataset, batch_size=1024):\n",
    "    \"\"\"\n",
    "Tests passed embedding model using all datasets and returns acurate factor.\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of Embedding class to be tested.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to test.\n",
    "Return:\n",
    "    Acurate factor (float).\n",
    "    \"\"\"\n",
    "    acurate = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        for y in tqdm(loader):\n",
    "            y = y.to(DEVICE)\n",
    "            X = model.to_dense(y)\n",
    "            pred = model(X)\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "    result = acurate/len(dataset)\n",
    "    return  result"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# dir_name = MAIN_DATA_DIR+'/models/'+ datetime.now().strftime(\"training_words_batch_%d_%m_%Y___%H_%M\")\n",
    "# dir_name = MAIN_DATA_DIR+'/models/training_words_batch_11_05_2022___08_59'"
   ],
   "metadata": {
    "id": "yRl3dg_h55G9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652364818632,
     "user_tz": -120,
     "elapsed": 643,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 100,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# embedding = Embedding(corpus_size=len(corpus), embedding_size=64, dropout_factor=0.18, sizes=[128, 512])\n",
    "# embedding = embedding.to(DEVICE)\n",
    "# words_batch = WordsBatch(embedding, hidden_state_size=96, dropout_factor=0.18, sequence_length=6, dense_layer_size=256)\n",
    "# words_batch = words_batch.to(DEVICE)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zLxNs7ycA9pU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745043,
     "user_tz": -120,
     "elapsed": 23,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GqfmO4zKt_w7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745044,
     "user_tz": -120,
     "elapsed": 23,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "# results = words_batch.info()\n",
    "# results['mse_initial'] = test_words_batch(words_batch, words_batch_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# words_batch = WordsBatch.load('/content/drive/MyDrive/data_harry_potter/models/training_words_batch_12_05_2022___09_22/words_batch_epoch_50.pth')\n",
    "# words_batch = words_batch.to(DEVICE)\n",
    "# with open(r'/content/drive/MyDrive/data_harry_potter/models/training_words_batch_12_05_2022___09_22/results.pickle', 'rb') as file:\n",
    "#   results = pickle.load(file)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "iWMiZct-A9pU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652365019260,
     "user_tz": -120,
     "elapsed": 489,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# for i in range(51, 64):\n",
    "#   results.pop(f'mse_after_epoch_{i}')"
   ],
   "metadata": {
    "id": "vL8qnSw0HeMO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652364680757,
     "user_tz": -120,
     "elapsed": 4,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 96,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# optimizer_words_batch = optim.Adam(words_batch.parameters())\n",
    "# # results['optimizer_type'] = 'Adam'"
   ],
   "metadata": {
    "id": "_87OZ0f8aMMD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345745046,
     "user_tz": -120,
     "elapsed": 22,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# optimizer_words_batch = optim.SGD(words_batch.parameters(), lr=0.5)"
   ],
   "metadata": {
    "id": "cwVcbBq51alZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652364860947,
     "user_tz": -120,
     "elapsed": 614,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train_words_batch(words_batch, words_batch_datasets, batch_size=16, epochs=10,\n",
    "#                   results=results, start_epoch=51, optimizer=optimizer_words_batch,\n",
    "#                   saves_dir=dir_name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJJugYoznzQi",
    "outputId": "19cadee5-6063-4d71-9625-af43b2ca24a9",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_initial = Embedding(corpus_size=len(corpus), embedding_size=64, dropout_factor=0.18, sizes=[128, 512])\n",
    "embedding_initial = embedding_initial.to(DEVICE)\n",
    "words_batch0 = WordsBatch.load('/content/drive/MyDrive/data_harry_potter/models/training_words_batch_11_05_2022___08_59/words_batch_epoch_0.pth')\n",
    "words_batch34 = WordsBatch.load('/content/drive/MyDrive/data_harry_potter/models/training_words_batch_11_05_2022___08_59/words_batch_epoch_34.pth')\n",
    "words_batch61 = WordsBatch.load('/content/drive/MyDrive/data_harry_potter/models/training_words_batch_13_05_2022___09_19/words_batch_epoch_61.pth')\n",
    "words_batch93 = WordsBatch.load('/content/drive/MyDrive/data_harry_potter/models/training_words_batch_13_05_2022___13_29/words_batch_epoch_93.pth')"
   ],
   "metadata": {
    "id": "jEH--OrlcZCc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652345906012,
     "user_tz": -120,
     "elapsed": 7089,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def check_distances(words: List[Tuple[str, str]], corpus: Corpus, embedding:Embedding):\n",
    "  for word1, word2 in words:  \n",
    "    dist = count_distance(word1, word2, corpus, embedding)\n",
    "    print(f\"    Distance between words '{word1}' and '{word2}' is {dist}\")"
   ],
   "metadata": {
    "id": "9QxvOAD4jv9Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# words = [('cat', 'dog'), ('cat', 'going'), ('hermione', 'water'), ('hermione', 'harry'),\n",
    "#          ('is', 'being'), ('is', 'was'), ('hagrid', 'dumbledore'), ('snake', 'snakes'),\n",
    "#          ('book', 'brilliant'), ('food', 'voldemort'), ('girl', 'boy'), ('girs', 'boys')]"
   ],
   "metadata": {
    "id": "Rb-kwtGHgWyB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652347216645,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# print(\"For initial words_batch: \")\n",
    "# check_distances(words, corpus, embedding_initial)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZpJC6M1hDzw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652347217471,
     "user_tz": -120,
     "elapsed": 4,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "e732d6c5-f4f8-4131-97e8-3916276d2941"
   },
   "execution_count": 79,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For initial words_batch: \n",
      "    Distance between words 'cat' and 'dog' is 97.79200744628906\n",
      "    Distance between words 'cat' and 'going' is 88.8342056274414\n",
      "    Distance between words 'hermione' and 'water' is 169.3144073486328\n",
      "    Distance between words 'hermione' and 'harry' is 89.15751647949219\n",
      "    Distance between words 'is' and 'being' is 105.0751953125\n",
      "    Distance between words 'is' and 'was' is 93.20569610595703\n",
      "    Distance between words 'hagrid' and 'dumbledore' is 124.45855712890625\n",
      "    Distance between words 'snake' and 'snakes' is 162.002197265625\n",
      "    Distance between words 'book' and 'brilliant' is 101.6808090209961\n",
      "    Distance between words 'food' and 'voldemort' is 142.20803833007812\n",
      "    Distance between words 'girl' and 'boy' is 119.31204223632812\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# print(\"For words_batch0: \")\n",
    "# check_distances(words, corpus, words_batch0.embedding)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onbN9GIKdmuN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652347218587,
     "user_tz": -120,
     "elapsed": 11,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "afaf6397-9158-45ef-93b9-16edb46ca5c3"
   },
   "execution_count": 80,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For words_batch0: \n",
      "    Distance between words 'cat' and 'dog' is 188.5636749267578\n",
      "    Distance between words 'cat' and 'going' is 159.94679260253906\n",
      "    Distance between words 'hermione' and 'water' is 124.99183654785156\n",
      "    Distance between words 'hermione' and 'harry' is 87.04646301269531\n",
      "    Distance between words 'is' and 'being' is 155.7615203857422\n",
      "    Distance between words 'is' and 'was' is 111.06465148925781\n",
      "    Distance between words 'hagrid' and 'dumbledore' is 114.62118530273438\n",
      "    Distance between words 'snake' and 'snakes' is 136.7913055419922\n",
      "    Distance between words 'book' and 'brilliant' is 134.72833251953125\n",
      "    Distance between words 'food' and 'voldemort' is 121.21263885498047\n",
      "    Distance between words 'girl' and 'boy' is 93.92500305175781\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# print(\"For words_batch34: \")\n",
    "# check_distances(words, corpus, words_batch34.embedding)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lk8BRKLlgUMS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652347218588,
     "user_tz": -120,
     "elapsed": 8,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "6547e1be-9a44-4437-9f8c-349a04fea7e5"
   },
   "execution_count": 81,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For words_batch0: \n",
      "    Distance between words 'cat' and 'dog' is 180.727294921875\n",
      "    Distance between words 'cat' and 'going' is 166.03314208984375\n",
      "    Distance between words 'hermione' and 'water' is 103.7860336303711\n",
      "    Distance between words 'hermione' and 'harry' is 37.58721160888672\n",
      "    Distance between words 'is' and 'being' is 136.8767852783203\n",
      "    Distance between words 'is' and 'was' is 71.04869079589844\n",
      "    Distance between words 'hagrid' and 'dumbledore' is 58.26959991455078\n",
      "    Distance between words 'snake' and 'snakes' is 131.8248291015625\n",
      "    Distance between words 'book' and 'brilliant' is 129.81053161621094\n",
      "    Distance between words 'food' and 'voldemort' is 115.80278015136719\n",
      "    Distance between words 'girl' and 'boy' is 86.43670654296875\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# print(\"For words_batch61: \")\n",
    "# check_distances(words, corpus, words_batch61.embedding)"
   ],
   "metadata": {
    "id": "OhiioQtwh0BI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652347094398,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"For words_batch93: \")\n",
    "# check_distances(words, corpus, words_batch93.embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}