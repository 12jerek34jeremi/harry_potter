{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#author: Jedrzej Chmiel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from words_batch_dataset import WordsBatchDataset\n",
    "from one_item_dataset import OneItemDataset\n",
    "from corpus import Corpus\n",
    "from embedding import Embedding\n",
    "from words_batch import WordsBatch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "MAIN_DATA_DIR ='data'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "one_item_dataset = OneItemDataset(len(corpus))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "words_batch_datasets = [WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_'+str(i)+'_prepared.txt', corpus.dictionary, sequence_length=6)\n",
    "                        for i in range(1,8)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "words_batch_dataset = WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_1_prepared.txt', corpus.dictionary, sequence_length=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "embedding = Embedding(corpus_size = len(corpus), embedding_size=128, dropout_factor=0.1)\n",
    "embedding = embedding.to(DEVICE)\n",
    "words_batch = WordsBatch(embedding, hidden_state_size=256, dropout_factor=0.1, sequence_length=6)\n",
    "words_batch = words_batch.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "optimizer_word_batch = optim.Adam(words_batch.parameters())\n",
    "optimizer_encoding = optim.Adam(embedding.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "mses = []\n",
    "acurate_factors = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def train_embedding(model:WordsBatch, datasets: list[OneItemDataset], batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None) -> list[float]:\n",
    "    \"\"\"\n",
    "This function trains words_batch model. It uses MSE loss function to do this.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of WordsBatch class.\n",
    "    datasets:\n",
    "        List of all datasets on which model will be trained. First function uses all inputs from first dataset,\n",
    "        then all input from second dataset, ..., then all outputs from last dataset. This is the end of first epoch.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of MSEs before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.MSELoss()\n",
    "    mse = 0.0\n",
    "    total_length = float(sum([len(dataset) for dataset in datasets]))\n",
    "    mses = []\n",
    "    loaders = [DataLoader(dataset, batch_size, shuffle=True) for dataset in datasets]\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.path.makedir(saves_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        mse = 0.0\n",
    "        for i,loader in enumerate(loaders):\n",
    "            print(f\"Epoch {epoch}/{epochs}, dataset {i+1}/7\")\n",
    "            for X, y in tqdm(loader, desc=\"batch: \"):\n",
    "                X = X.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    y = model.embedding.to_dense(y).to(DEVICE)\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                mse+=loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        mse /= total_length\n",
    "        print(f\"MSE is: {mse}\")\n",
    "        mses.append(mse)\n",
    "        model.save(saves_dir+'/'+f\"words_batch_epoch_{epoch}.pth\")\n",
    "    return mses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def test_embedding(model:WordsBatch, datasets: list[OneItemDataset]) -> float:\n",
    "    \"\"\"\n",
    "Tests passed word batch model using all datasets and returns MSE\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of WordsBatch class to be tested.\n",
    "    datasets:\n",
    "        List of onjects of WordsBatchDataset class. Datasets on which model will be tested.\n",
    "    \"\"\"\n",
    "    mse = 0.0\n",
    "    total_length = float(sum([len(dataset) for dataset in datasets]))\n",
    "    loss_function = nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dataset in tqdm(datasets):\n",
    "            for X,y in dataset:\n",
    "                X = torch.unsqueeze(X, dim=0).to(DEVICE)\n",
    "                y = model.embedding.to_dense(torch.unsqueeze(y, dim=0)).to(DEVICE)\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                mse += loss.item()\n",
    "    mse /= total_length\n",
    "    print(f\"MSE is {mse}\")\n",
    "    return mse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def train_encoding(model: Embedding, dataset: OneItemDataset, batch_size: int, epochs: int, optimizer: optim.Optimizer, saves_dir: str = None) -> list[float]:\n",
    "    \"\"\"\n",
    "This function trains encoding part of Embedding class object. It uses CrossEntropyLoss.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of Embedding class.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to train.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of acurate factor before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    acurate = 0\n",
    "    acurates = []\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.path.makedir(saves_dir)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        for y in tqdm(loader, desc=\"batch: \"):\n",
    "            with torch.no_grad():\n",
    "                X = embedding.to_dense(y)\n",
    "            pred = model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "\n",
    "        model.save(saves_dir+'/'+f\"embedding_epoch_{epoch}.pth\")\n",
    "    print(f\"Acurate: {acurate/len(dataset)}\")\n",
    "    acurates.append(acurate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def test_encoding(model, dataset):\n",
    "    \"\"\"\n",
    "Tests passed embedding model using all datasets and returns acurate factor.\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of Embedding class to be tested.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to test.\n",
    "Return:\n",
    "    Acurate factor (float).\n",
    "    \"\"\"\n",
    "    acurate = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for y in tqdm(dataset):\n",
    "            y = torch.unsqueeze(y, dim=0).to(DEVICE)\n",
    "            X = model.to_dense(y)\n",
    "            pred = model(X)\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "    result = acurate/len(dataset)\n",
    "    print(f\"Acurate fraction: {result}\")\n",
    "    return  result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_mses = train_embedding(words_batch, words_batch_datasets, batch_size=16, epochs=10, optimizer=optimizer_word_batch)\n",
    "mses += new_mses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_mse = test_embedding(words_batch, words_batch_datasets)\n",
    "mses.append(new_mse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_acurates =  train_encoding(embedding, one_item_dataset, batch_size=16, epochs=10, optimizer=optimizer_word_batch)\n",
    "acurate_factors += new_acurates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_acurate = test_encoding(embedding, one_item_dataset)\n",
    "acurate_factors.append(new_acurate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "with open(MAIN_DATA_DIR+'/results/mses.pickle', 'wb') as file:\n",
    "    pickle.dump(mses, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "with open(MAIN_DATA_DIR+'/results/acurate_factors.pickle', 'wb') as file:\n",
    "    pickle.dump(acurate_factors, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#author: Jedrzej Chmiel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}