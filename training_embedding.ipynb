{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ChpAIG52t_w0"
   },
   "source": [
    "#author: Jedrzej Chmiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFwFtxvSlrxX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652101940948,
     "user_tz": -120,
     "elapsed": 4033,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "10ca10c9-6ab0-4ede-ae86-6d766139a1a0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk==3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "1UrRDgIbt_w3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652101951822,
     "user_tz": -120,
     "elapsed": 6241,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from words_batch_dataset import WordsBatchDataset\n",
    "from one_item_dataset import OneItemDataset\n",
    "from corpus import Corpus\n",
    "from embedding import Embedding\n",
    "from words_batch import WordsBatch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXXPsWOOjh8l",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652046128797,
     "user_tz": -120,
     "elapsed": 707,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "96c161c2-ac2c-4e01-d631-3ccf7ac726af",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqLBn7gtuMyw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652046130556,
     "user_tz": -120,
     "elapsed": 307,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "4bb737a0-0532-4519-a5b3-dc669635b532",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bnej8iGMt_w4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652046145818,
     "user_tz": -120,
     "elapsed": 14970,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "82f12895-556d-448c-9f16-c40a4fe9ae7a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WuhW91W6vMRp"
   },
   "outputs": [],
   "source": [
    "# MAIN_DATA_DIR = 'drive/MyDrive/data_harry_potter'\n",
    "MAIN_DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ucAT7QTAt_w4"
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "one_item_dataset = OneItemDataset(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0svU9iY0t_w4"
   },
   "outputs": [],
   "source": [
    "words_batch_datasets = [WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_'+str(i)+'_prepared.txt', corpus.dictionary, sequence_length=6)\n",
    "                        for i in range(1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZJGrrpG5t_w4"
   },
   "outputs": [],
   "source": [
    "embedding = Embedding(corpus_size = len(corpus), embedding_size=128, dropout_factor=0.18)\n",
    "embedding = embedding.to(DEVICE)\n",
    "words_batch = WordsBatch(embedding, hidden_state_size=256, dropout_factor=0.18, sequence_length=6)\n",
    "words_batch = words_batch.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbxg1c0zt_w5"
   },
   "outputs": [],
   "source": [
    "WORDS_BATCH_LEARNING_RATE = 0.001\n",
    "optimizer_word_batch = optim.Adam(words_batch.parameters())\n",
    "optimizer_encoding = optim.Adam(embedding.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def test_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size=2048) -> float:\n",
    "    \"\"\"\n",
    "Tests passed word batch model using all datasets and returns MSE\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of WordsBatch class to be tested.\n",
    "    datasets:\n",
    "        List of onjects of WordsBatchDataset class. Datasets on which model will be tested.\n",
    "    \"\"\"\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "      loss_function = nn.MSELoss(reduction='sum')\n",
    "      for dataset in datasets:\n",
    "        loader = DataLoader(dataset, batch_size=1024, shuffle=False, drop_last=False)\n",
    "        for X, y in loader:\n",
    "          X = X.to(DEVICE)\n",
    "          y = y.to(DEVICE)\n",
    "          y = model.embedding.to_dense(y)\n",
    "          pred = model(X)\n",
    "          loss = loss_function(pred, y)\n",
    "          mse +=  loss.item()\n",
    "    \n",
    "    total_length = sum([len(dataset) for dataset in datasets])\n",
    "    mse = mse / total_length\n",
    "    return mse"
   ],
   "metadata": {
    "id": "t_N5C7eOmLju"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjVCsVrit_w5"
   },
   "outputs": [],
   "source": [
    "def train_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None, results:dict[str, int or float] = None,\n",
    "                    start_epoch:int = 0):\n",
    "    \"\"\"\n",
    "This function trains words_batch model. It uses MSE loss function to do this.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of WordsBatch class.\n",
    "    datasets:\n",
    "        List of all datasets on which model will be trained. First function uses all inputs from first dataset,\n",
    "        then all input from second dataset, ..., then all outputs from last dataset. This is the end of first epoch.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of MSEs before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.MSELoss()\n",
    "    mse = 0.0\n",
    "    loaders = [DataLoader(dataset, batch_size, shuffle=True) for dataset in datasets]\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "        with open(saves_dir+'/results.pickkle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "        with open(saves_dir+'/results.txt', 'wt') as file:\n",
    "            for key, item in results.items():\n",
    "                file.write(key + ': ' + str(item) + '\\n')\n",
    "\n",
    "    end_epoch = start_epoch+epochs\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        mse = 0.0\n",
    "        for i,loader in enumerate(loaders):\n",
    "            print(f\"Epoch {epoch}/{end_epoch}, dataset {i+1}/7\")\n",
    "            for X, y in tqdm(loader, desc=\"batch: \"):\n",
    "                X = X.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    y = model.embedding.to_dense(y.to(DEVICE))\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                mse+=loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if saves_dir is not None:\n",
    "            model.save(saves_dir+'/'+f\"words_batch_epoch_{epoch}.pth\")\n",
    "            if results is not None:\n",
    "                mse = test_words_batch(model, datasets)\n",
    "                print(f\"MSE is: {mse}\")\n",
    "                results[f'mse_after_epoch_{epoch}'] = mse\n",
    "                with open(saves_dir+'/results.pickkle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "                with open(saves_dir+'/results.txt', 'at') as file:\n",
    "                    file.write(f'mse_after_epoch_{epoch} : {mse}\\n')\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVePgaVIt_w6"
   },
   "outputs": [],
   "source": [
    "def train_encoding(model: Embedding, dataset: OneItemDataset, batch_size: int, epochs: int,\n",
    "                   optimizer: optim.Optimizer, saves_dir: str = None, with_training:bool = True) -> List[float]:\n",
    "    \"\"\"\n",
    "This function trains encoding part of Embedding class object. It uses CrossEntropyLoss.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of Embedding class.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to train.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of acurate factor before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    acurate = 0\n",
    "    acurates = []\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        for y in tqdm(loader, desc=\"batch: \"):\n",
    "            with torch.no_grad():\n",
    "                X = embedding.to_dense(y)\n",
    "            pred = model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "\n",
    "        model.save(saves_dir+'/'+f\"embedding_epoch_{epoch}.pth\")\n",
    "    print(f\"Acurate: {acurate/len(dataset)}\")\n",
    "    acurates.append(acurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6LdFTmnt_w6"
   },
   "outputs": [],
   "source": [
    "def test_encoding(model, dataset):\n",
    "    \"\"\"\n",
    "Tests passed embedding model using all datasets and returns acurate factor.\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of Embedding class to be tested.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to test.\n",
    "Return:\n",
    "    Acurate factor (float).\n",
    "    \"\"\"\n",
    "    acurate = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for y in tqdm(dataset):\n",
    "            y = torch.unsqueeze(y, dim=0).to(DEVICE)\n",
    "            X = model.to_dense(y)\n",
    "            pred = model(X)\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "    result = acurate/len(dataset)\n",
    "    print(f\"Acurate fraction: {result}\")\n",
    "    return  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqfmO4zKt_w7"
   },
   "outputs": [],
   "source": [
    "dir_name = MAIN_DATA_DIR+'/models/'+ datetime.now().strftime(\"training_embedding_%d_%m_%Y___%H_%M\")\n",
    "results = words_batch.info()\n",
    "results['initial_optimizer_learning_rate'] = WORDS_BATCH_LEARNING_RATE\n",
    "results['mse_initial'] = test_words_batch(words_batch, words_batch_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_words_batch(words_batch, words_batch_datasets, batch_size=16, epochs=7,\n",
    "                otimizer=optimizer_word_batch, saves_dir=dir_name, results=results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "kCJZpqlq9BPn",
    "executionInfo": {
     "status": "error",
     "timestamp": 1652102543524,
     "user_tz": -120,
     "elapsed": 845,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "cff76e16-ea6b-4645-8389-d6d69af0a3d1",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "author: Jędrzej Chmiel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}