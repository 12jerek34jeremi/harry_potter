{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ChpAIG52t_w0"
   },
   "source": [
    "#author: Jedrzej Chmiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFwFtxvSlrxX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126319865,
     "user_tz": -120,
     "elapsed": 3973,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "b2f3f8c6-dde6-495b-903b-94cb214f3a2d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install nltk==3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1UrRDgIbt_w3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126319866,
     "user_tz": -120,
     "elapsed": 11,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from words_batch_dataset import WordsBatchDataset\n",
    "from one_item_dataset import OneItemDataset\n",
    "from corpus import Corpus\n",
    "from embedding import Embedding\n",
    "from words_batch import WordsBatch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXXPsWOOjh8l",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126320205,
     "user_tz": -120,
     "elapsed": 349,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "c5df33b3-b003-4e5a-a8da-9df573adacb9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqLBn7gtuMyw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126320206,
     "user_tz": -120,
     "elapsed": 8,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "18f653f3-dbc8-4758-a685-a01e5c6d8a7e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bnej8iGMt_w4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126321906,
     "user_tz": -120,
     "elapsed": 1706,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "a768b04f-daec-4bba-b20a-421d34e73d98"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WuhW91W6vMRp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126321907,
     "user_tz": -120,
     "elapsed": 4,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "MAIN_DATA_DIR = 'drive/MyDrive/data_harry_potter'\n",
    "# MAIN_DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ucAT7QTAt_w4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126321907,
     "user_tz": -120,
     "elapsed": 4,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "one_item_dataset = OneItemDataset(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0svU9iY0t_w4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126400552,
     "user_tz": -120,
     "elapsed": 78648,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "words_batch_datasets = [WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_'+str(i)+'_prepared.txt', corpus.dictionary, sequence_length=6)\n",
    "                        for i in range(1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def test_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size=2048) -> float:\n",
    "    \"\"\"\n",
    "Tests passed word batch model using all datasets and returns MSE\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of WordsBatch class to be tested.\n",
    "    datasets:\n",
    "        List of onjects of WordsBatchDataset class. Datasets on which model will be tested.\n",
    "    \"\"\"\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "      loss_function = nn.MSELoss(reduction='sum')\n",
    "      for dataset in datasets:\n",
    "        loader = DataLoader(dataset, batch_size=1024, shuffle=False, drop_last=False)\n",
    "        for X, y in loader:\n",
    "          X = X.to(DEVICE)\n",
    "          y = y.to(DEVICE)\n",
    "          y = model.embedding.to_dense(y)\n",
    "          pred = model(X)\n",
    "          loss = loss_function(pred, y)\n",
    "          mse +=  loss.item()\n",
    "    \n",
    "    total_length = sum([len(dataset) for dataset in datasets])\n",
    "    mse = mse / total_length\n",
    "    return mse"
   ],
   "metadata": {
    "id": "t_N5C7eOmLju",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126401296,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qjVCsVrit_w5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126633293,
     "user_tz": -120,
     "elapsed": 331,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "def train_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None, results:Dict[str, int or float] = None,\n",
    "                    start_epoch:int = 0):\n",
    "    \"\"\"\n",
    "This function trains words_batch model. It uses MSE loss function to do this.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of WordsBatch class.\n",
    "    datasets:\n",
    "        List of all datasets on which model will be trained. First function uses all inputs from first dataset,\n",
    "        then all input from second dataset, ..., then all outputs from last dataset. This is the end of first epoch.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of MSEs before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.MSELoss()\n",
    "    loaders = [DataLoader(dataset, batch_size, shuffle=True) for dataset in datasets]\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "        with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "        with open(saves_dir+'/results.txt', 'wt') as file:\n",
    "            for key, item in results.items():\n",
    "                file.write(key + ': ' + str(item) + '\\n')\n",
    "\n",
    "    end_epoch = start_epoch+epochs\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        for i,loader in enumerate(loaders):\n",
    "            print(f\"Epoch {epoch}/{end_epoch}, dataset {i+1}/7\")\n",
    "            for X, y in tqdm(loader, desc=\"batch: \"):\n",
    "                X = X.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    y = model.embedding.to_dense(y.to(DEVICE))\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if saves_dir is not None:\n",
    "            model.save(saves_dir+'/'+f\"words_batch_epoch_{epoch}.pth\")\n",
    "            if results is not None:\n",
    "                mse = test_words_batch(model, datasets)\n",
    "                print(f\"MSE is: {mse}\")\n",
    "                results[f'mse_after_epoch_{epoch}'] = mse\n",
    "                with open(saves_dir+'/results.pickkle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "                with open(saves_dir+'/results.txt', 'at') as file:\n",
    "                    file.write(f'mse_after_epoch_{epoch} : {mse}\\n')\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HVePgaVIt_w6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126401297,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "def train_encoding(model: Embedding, dataset: OneItemDataset, batch_size: int, epochs: int,\n",
    "                   optimizer: optim.Optimizer, saves_dir: str = None, with_training:bool = True) -> List[float]:\n",
    "    \"\"\"\n",
    "This function trains encoding part of Embedding class object. It uses CrossEntropyLoss.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of Embedding class.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to train.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of acurate factor before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    acurate = 0\n",
    "    acurates = []\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        for y in tqdm(loader, desc=\"batch: \"):\n",
    "            with torch.no_grad():\n",
    "                X = embedding.to_dense(y)\n",
    "            pred = model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "\n",
    "        model.save(saves_dir+'/'+f\"embedding_epoch_{epoch}.pth\")\n",
    "    print(f\"Acurate: {acurate/len(dataset)}\")\n",
    "    acurates.append(acurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M6LdFTmnt_w6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126401297,
     "user_tz": -120,
     "elapsed": 8,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "def test_encoding(model, dataset):\n",
    "    \"\"\"\n",
    "Tests passed embedding model using all datasets and returns acurate factor.\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of Embedding class to be tested.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to test.\n",
    "Return:\n",
    "    Acurate factor (float).\n",
    "    \"\"\"\n",
    "    acurate = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for y in tqdm(dataset):\n",
    "            y = torch.unsqueeze(y, dim=0).to(DEVICE)\n",
    "            X = model.to_dense(y)\n",
    "            pred = model(X)\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "    result = acurate/len(dataset)\n",
    "    print(f\"Acurate fraction: {result}\")\n",
    "    return  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "embedding = Embedding(corpus_size=len(corpus), embedding_size=64, dropout_factor=0.18, sizes=[128, 512])\n",
    "embedding = embedding.to(DEVICE)\n",
    "words_batch = WordsBatch(embedding, hidden_state_size=96, dropout_factor=0.18, sequence_length=6, dense_layer_size=256)\n",
    "words_batch = words_batch.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GqfmO4zKt_w7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126401297,
     "user_tz": -120,
     "elapsed": 8,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "outputs": [],
   "source": [
    "dir_name = MAIN_DATA_DIR+'/models/'+ datetime.now().strftime(\"training_embedding_%d_%m_%Y___%H_%M\")\n",
    "results = words_batch.info()\n",
    "results['mse_initial'] = test_words_batch(words_batch, words_batch_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results, dir_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# words_batch = WordsBatch.load(dir_name+'/words_batch_epoch_6.pth')\n",
    "# words_batch = words_batch.to(DEVICE)\n",
    "# with open(dir_name+'/results.pickle', 'rb') as file:\n",
    "#   results = pickle.load(file)\n",
    "# results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# dir_name = MAIN_DATA_DIR+'/models/training_words_batch_09_05_2022___15_58'"
   ],
   "metadata": {
    "id": "-eBJZlY7aMS5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652126723907,
     "user_tz": -120,
     "elapsed": 316,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    }
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer_words_batch = optim.Adam(words_batch.parameters())\n",
    "results['optimizer_type'] = 'Adam'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_87OZ0f8aMMD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652127086148,
     "user_tz": -120,
     "elapsed": 1363,
     "user": {
      "displayName": "Jędrzej Chmiel",
      "userId": "01032980401837788346"
     }
    },
    "outputId": "68dfd0b1-92ff-45a4-f672-905a1dea72a4"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train_words_batch(words_batch, words_batch_datasets, batch_size=16, epochs=15,\n",
    "#               optimizer=optimizer_words_batch, saves_dir=dir_name, results=results,\n",
    "#               start_epoch = 0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld4uhbV7aLIN",
    "outputId": "4da78d88-6578-487e-d76b-fee7ad19e954",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}