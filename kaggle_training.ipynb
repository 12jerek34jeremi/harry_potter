{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#author: Jedrzej Chmiel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from typing import Dict\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "This class can be used to transform a token (like 1513) to a dense vector (see to_dense method) and to transform a dense\n",
    "vector to an item (see get_word_propabilities method).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_size: int, embedding_size: int,\n",
    "                 dropout_factor: float, sizes = [512, 1024, 2048]):\n",
    "        \"\"\"\n",
    "    Creates an Embedding class object.\n",
    "    Parameters:\n",
    "        corpus_size: the size of corpus, how many words there are in dictionary\n",
    "        embedding_size: the lenght of a dense vector which will represent a word\n",
    "        dropout_factor: the dropout factor used in each layer of encoding network.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.__embedding = nn.Embedding(corpus_size, embedding_size)\n",
    "\n",
    "        self.__encoding = nn.ModuleList()\n",
    "        for input_dim, output_dim in zip([embedding_size]+sizes[:-1], sizes):\n",
    "            self.__encoding.extend([nn.Linear(input_dim, output_dim), nn.ReLU(), nn.Dropout(dropout_factor)])\n",
    "        self.__encoding.append(nn.Linear(sizes[-1], corpus_size))\n",
    "\n",
    "        self.corpus_size = corpus_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_factor = dropout_factor\n",
    "        self.sizes = sizes\n",
    "\n",
    "    def to_dense(self, tokens: torch.Tensor):\n",
    "        \"\"\"\n",
    "    Transform tokens to dense vecor.\n",
    "    Parameters:\n",
    "        tokens:\n",
    "            The tensor of shape (N,) where N is number of tokens (a batch size).\n",
    "    Return:\n",
    "        The tensor of shape (N, e_s), where e_s is embedding size (length of vector representing on word) and N is batch\n",
    "        size (length of tokens)\n",
    "        \"\"\"\n",
    "        return self.__embedding(tokens)\n",
    "\n",
    "    def words_probabilities(self, dense_embedding: torch.Tensor):\n",
    "        \"\"\"\n",
    "    Used to transform dense vector (embedding) to tokens. Returns the tensor representing the propability that given\n",
    "    vector represents given word.\n",
    "    Parameters:\n",
    "        dense_embedding:\n",
    "            A tensor of shape (N, e_s), where here e_s is embedding size (length of vector representing on word) and N is\n",
    "            batch size (number of words).\n",
    "        Return:\n",
    "            A tensor of shape (N, c_s) where c_s is corpus size (number of all available words) and N is batch size\n",
    "            (number of passed words). If dense_embedding is of shape (1, e_s) and returened tensor lookes like this:\n",
    "            [[0.01, 0.02, 0.93, 0.1, 0.003, ..., 0.01]], then it means that for 93% passed dense_vector represent word\n",
    "            of id 2 (because 0.93 is at position [0,2]).\n",
    "        \"\"\"\n",
    "        return f.Softmax(self.__encoding(dense_embedding), dim=-1)\n",
    "\n",
    "    def forward(self, dense_embedding: torch.Tensor):\n",
    "        \"\"\"\n",
    "    Used to transform dense vector (embedding) to log propabilities of each token. Returns the tensor representing the\n",
    "    log probability that given vector represents given word.\n",
    "    Parameters:\n",
    "        dense_embedding:\n",
    "            A tensor of shape (N, e_s), where here e_s is embedding size (length of vector representing on word) and N\n",
    "            is batch size (number of words).\n",
    "        Return:\n",
    "            A tensor of shape (N, c_s) where c_s is corpus size (number of all available words) and N is batch size\n",
    "            (number of passed words). If dense_embedding is of shape (1,5) and returned tensor looks like this:\n",
    "            [[-2.5, -145.0, -1.0, -2.0, -0.1, -0.2]], then it means that log probability of the fact that passed vector\n",
    "            represent word of if 1 is -145.0.\n",
    "        \"\"\"\n",
    "        return f.log_softmax(self.__encoding(dense_embedding), dim=-1)\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        filepath: str,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "    Saves object in file described by filepath. The directory in which file is gonna to be must exist before calling\n",
    "    this function. If file doesn't exist it will be created, otherwise it will be truncated. If an problem was\n",
    "    encounter while trying to save this model in a given, file function returns False. Otherwise, function returns True.\n",
    "    In file there is also saved the embedding used by this object.\n",
    "    Parameters:\n",
    "        filepath:\n",
    "            The path of the file in which this model will be saved.\n",
    "    Return:\n",
    "        True in case of success, False in case of failure.\n",
    "        \"\"\"\n",
    "        parameters_dict = self.info()\n",
    "        parameters_dict[\"state_dict\"] = self.state_dict()\n",
    "        try:\n",
    "            torch.save(parameters_dict, filepath)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Sorry, an exception occurred while trying to save model to file {filepath}\"\n",
    "            )\n",
    "            return False\n",
    "\n",
    "    def info(self) -> Dict[str, int or float]:\n",
    "        parameters_dict = {\n",
    "            \"corpus_size\": self.corpus_size,\n",
    "            \"embedding_size\": self.embedding_size,\n",
    "            \"dropout_factor\": self.dropout_factor,\n",
    "            'sizes': self.sizes\n",
    "        }\n",
    "        return parameters_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath: str) -> 'Embedding':\n",
    "        \"\"\"\n",
    "    An static function to. Loads an embedding model from file and returns it. If any problems occur while trying to read\n",
    "    object from a file, function returns None.\n",
    "    Parameters:\n",
    "        filepath:\n",
    "            The filepath to object in which file is saved. It should have been created by save method of this class.\n",
    "    Return:\n",
    "        The Embedding class object in case of success. None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parameters_dict = torch.load(filepath)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Sorry, an exception occurred while trying to save model to file {filepath}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if 'sizes' in parameters_dict:\n",
    "            sizes = parameters_dict['sizes']\n",
    "        else:\n",
    "            sizes = [512,1024,2048]\n",
    "\n",
    "        embedding = Embedding(parameters_dict['corpus_size'],\n",
    "                              parameters_dict['embedding_size'],\n",
    "                              parameters_dict['dropout_factor'], sizes=sizes)\n",
    "        embedding.load_state_dict(parameters_dict['state_dict'])\n",
    "        return embedding\n",
    "\n",
    "#author: Jedrzej Chmiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Author: Jedrzej Chmiel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict\n",
    "\n",
    "class WordsBatch(nn.Module): #second version\n",
    "    \"\"\"A nural netwrok of given archicture:\n",
    "    Assume embedding_size is 128, corpus_size is 21371, hidden_state_size is 256 and dropout_factor is 0.1 and\n",
    "    sequence_length is 3.\n",
    "\n",
    "                    Fully connected layer of size 128.                           -\n",
    "                                    |                                               |\n",
    "                    Fully concatenation layer of length 1024, ReLU activation       |\n",
    "                                 and 0.1 dropout.                                   ----- the tail\n",
    "                                       |                                            |\n",
    "                                    tail_input (512,)                            -\n",
    "                                       |\n",
    "                                       | (concatenation)\n",
    "                    hidden_state  -----|-----  hidden_state\n",
    "                        (256,)                 (256,)\n",
    "                          |                     |\n",
    "    LSTM------LSTM------LSTM                   LSTM------LSTM------LSTM\n",
    "      |         |         |                     |         |         |\n",
    "    LSTM------LSTM------LSTM                   LSTM------LSTM------LSTM\n",
    "      |         |         |                     |         |         |\n",
    "    dense     dense     dense                  dense     dense     dense\n",
    "    vector    vector    vector                 vector    vector    vector\n",
    "    (128,)    (128,)    (128,)                 (128,)    (128,)    (128,)         <----shape of tensor\n",
    "      |         |         |                       |         |         |\n",
    "    token      token      token                token      token      token        <--- each token is one-item tensor\n",
    "      |         |          |                      |         |          |\n",
    "    Hogward     is         the                 school      for       wizzards\n",
    "\n",
    "    (We want to predict the word 'best' in sentance: Hogward is the best school for wizzards)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding: Embedding, hidden_state_size: int,\n",
    "                 dropout_factor: float, sequence_length: int, dense_layer_size: int = 1024):\n",
    "        \"\"\"\n",
    "    Parameters:\n",
    "        embedding:\n",
    "            The object of Embedding class. It will be used to convert tokens for dense vectors.\n",
    "        hidden_state_size:\n",
    "            The size of hidden_state in both layers of LSTM.\n",
    "        dropout_factor:\n",
    "            Dropout factor in LSTM layers and in tail in fully connected layers.\n",
    "        sequence_length:\n",
    "            How many words before and after will be used to predict the middle word. If sequence_length is 3 then\n",
    "            input to this model should be three words before and three words after.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        embedding_size = embedding.embedding_size\n",
    "        self.lstm_before = nn.LSTM(embedding_size,\n",
    "                                   hidden_state_size,\n",
    "                                   2,\n",
    "                                   dropout=dropout_factor,\n",
    "                                   batch_first=True)\n",
    "        self.lstm_after = nn.LSTM(embedding_size,\n",
    "                                  hidden_state_size,\n",
    "                                  2,\n",
    "                                  dropout=dropout_factor,\n",
    "                                  batch_first=True)\n",
    "        self.tail = nn.Sequential(nn.Linear(hidden_state_size * 2, dense_layer_size),\n",
    "                                  nn.ReLU(), nn.Dropout(dropout_factor),\n",
    "                                  nn.Linear(dense_layer_size, embedding_size))\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.dropout_factor = dropout_factor\n",
    "        self.dense_layer_size = dense_layer_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "    Returns a dense vector (embedding) representing the predicted middle word.\n",
    "    Parameters:\n",
    "        input:\n",
    "            The tensor of shape (N, 2*s_l), where N is batch size and s_l is sequence_length. First part of second axis\n",
    "             are words before word which is to be predicted and second part of second axis are words after word which is\n",
    "             to be predicted.\n",
    "    Return:\n",
    "        The tensor of shape (N, e_s), where N is batch size and e_s is embedding_size.\n",
    "\n",
    "    Suppose sequence_length is 3, batch size is 1, embedding_size is 128 and we want to predict the word \"best\" in\n",
    "    sentance: \"hogward is the best school for wizzards\". Suppose those are ids of words used in this sentance:\n",
    "    {'hogward': 123, 'is': 34, 'the': 13645, 'best': 7452, 'school': 15123, 'for': 541, 'wizzards': 231}.\n",
    "    To predict the middle word in that sentance we should pass to this function the following tensor:\n",
    "    [[123, 34, 13645, 231, 541, 15123]]. And we should get tensor of shape (1, 128) being the dense vector representing\n",
    "    the word 'best'.\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        input = self.embedding.to_dense(input)\n",
    "        _, (hiddens_before,\n",
    "            _) = self.lstm_before(input[:, :self.sequence_length, :])\n",
    "        _, (hiddens_after,\n",
    "            _) = self.lstm_after(input[:, self.sequence_length:, :])\n",
    "        return self.tail(\n",
    "            torch.stack([\n",
    "                torch.cat((hiddens_before[1, i], hiddens_after[1, i]), dim=0)\n",
    "                for i in range(batch_size)\n",
    "            ]))\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        filepath: str,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "    Saves object in file described by filepath. The directory in which file is gonna tobe must exist before calling\n",
    "    this function. If file doesn't exist it will be created, otherwise it will be truncated. If an problem was\n",
    "    encounter while trying to save this model in a given, file function returns False. Otherwise, function returns True.\n",
    "    Parameters:\n",
    "        filepath:\n",
    "            The path of the file in which this model will be saved.\n",
    "    Return:\n",
    "        True in case of success, False in case of failure.\n",
    "        \"\"\"\n",
    "        parameters_dict = self.info()\n",
    "        parameters_dict['words_batch_state_dict'] = self.state_dict()\n",
    "        parameters_dict['embedding_state_dict'] = self.embedding.state_dict()\n",
    "        try:\n",
    "            torch.save(parameters_dict, filepath)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Sorry, an exception occurred while trying to save model to file {filepath}\"\n",
    "            )\n",
    "            return False\n",
    "\n",
    "    def info(self) -> Dict[str, int or float]:\n",
    "        parameters_dict = {\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"hidden_state_size\": self.hidden_state_size,\n",
    "            \"words_batch_dropout_factor\": self.dropout_factor,\n",
    "            \"corpus_size\": self.embedding.corpus_size,\n",
    "            \"embedding_size\": self.embedding.embedding_size,\n",
    "            \"embedding_dropout_factor\": self.embedding.dropout_factor,\n",
    "            \"dense_layer_size\": self.dense_layer_size,\n",
    "            'embedding_sizes': self.embedding.sizes\n",
    "        }\n",
    "        return parameters_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath: str) -> 'WordsBatch':\n",
    "        \"\"\"\n",
    "    An static function to. Loads an embedding model from file and returns it. If any problems occur while trying to read\n",
    "    object from a file, function returns None.\n",
    "    Parameters:\n",
    "        filepath:\n",
    "            The filepath to object in which file is saved. It should have been created by save method of this class.\n",
    "    Return:\n",
    "        The Embedding class object in case of success. None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parameters_dict = torch.load(filepath)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Sorry, an exception occurred while trying to save model to file {filepath}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        if 'dense_layer_size' in parameters_dict:\n",
    "            dense_layer_size = parameters_dict['dense_layer_size']\n",
    "        else:\n",
    "            dense_layer_size = 1024\n",
    "        if 'embedding_sizes' in parameters_dict:\n",
    "            embedding_sizes = parameters_dict['embedding_sizes']\n",
    "        else:\n",
    "            embedding_sizes = [512,1024,2048]\n",
    "\n",
    "        embedding = Embedding(parameters_dict['corpus_size'],\n",
    "                              parameters_dict['embedding_size'],\n",
    "                              parameters_dict['embedding_dropout_factor'],\n",
    "                              sizes=embedding_sizes)\n",
    "        embedding.load_state_dict(parameters_dict['embedding_state_dict'])\n",
    "\n",
    "        words_batch = WordsBatch(embedding,\n",
    "                                 parameters_dict['hidden_state_size'],\n",
    "                                 parameters_dict['words_batch_dropout_factor'],\n",
    "                                 parameters_dict['sequence_length'],\n",
    "                                 dense_layer_size = dense_layer_size)\n",
    "        words_batch.load_state_dict(parameters_dict['words_batch_state_dict'])\n",
    "        return words_batch\n",
    "\n",
    "# Author: Jedrzej Chmiel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#author: Jedrzej Chmiel\n",
    "import pickle\n",
    "\n",
    "class Corpus:\n",
    "    \"\"\"\n",
    "This class can be used to change words to tokens and tokens to words. After creating an object of this class named\n",
    "'the_corpus' you can check what word has id 15432 using the_corpus[15432]. You can also check what is the id of word\n",
    "'cat' using the_corpus['cat'].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary_filepath: str):\n",
    "        \"\"\"\n",
    "    Creates object of class Corpus.\n",
    "    Parameters:\n",
    "        dictionary_filepath:\n",
    "            The file path to the dictionary[str, int] saved using pickle module. The dictionary should contain id's of\n",
    "            all necessary words. Exemplatory dictionary:\n",
    "            {\"cat\": 0, \"wizzard\": 1, \"grass\": 2}\n",
    "            Dictionary should not contain any id gaps! (If max id is 5 and min id is 0 one, then one word should be\n",
    "            assigned to each integer number belonging to <0, 5>). The lowest id should be 0. This function checks if\n",
    "            dictionary saved in passed file is correct. If there is an id gap in dictionary, function prints this\n",
    "            information on the screen and raises an exception.\n",
    "    This constructor creates a list of str. In this list under each index is word which id is this index. This list is\n",
    "    used then to quickly check what word has given id.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(dictionary_filepath, 'rb') as file:\n",
    "                self.dictionary = pickle.load(file)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"There was an error while trying to read dictionary from file: \",\n",
    "                dictionary_filepath)\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "        self.__length = len(self.dictionary)\n",
    "        words = [None for _ in range(self.__length)]\n",
    "        for word, word_id in self.dictionary.items():\n",
    "            words[word_id] = word\n",
    "\n",
    "        if None in words:\n",
    "            print(\"Dictionary saved in file: \", dictionary_filepath,\n",
    "                  \" has a id gap.\")\n",
    "            print(\"There is no word assigned to id: \", words.index(None))\n",
    "            raise Exception(\"Id gap in dictionary.\")\n",
    "            return None\n",
    "\n",
    "        self.__words = words\n",
    "\n",
    "    def __getitem__(self, index: str or int):\n",
    "        \"\"\"\n",
    "    If type of index is str function returns the id (int) assigned to index.\n",
    "    If type of index is int function returns the word (str) assigned to index.\n",
    "    If index is of any other type function raises an TypeError.\n",
    "    Parameters:\n",
    "        index:\n",
    "            word (str) (to get id) or id (int) (to get word)\n",
    "    Return:\n",
    "        word (str) or id (int)\n",
    "        \"\"\"\n",
    "        if isinstance(index, int):\n",
    "            return self.__words[index]\n",
    "        elif isinstance(index, str):\n",
    "            return self.__dictionary[index]\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Unsupported index type: \" + str(type(index)) +\n",
    "                \" (in __getitem__ function of Corpus class object)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Returns the number of words in dictionary. (max_id+1)\"\n",
    "        return self.__length\n",
    "\n",
    "#author: Jedrzej Chmiel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class WordsBatchDataset(Dataset):\n",
    "    \"\"\"DataSet for training word batch. One dataset is one book.\n",
    "    To train over all 7 books you need to create 7 datasets.  Uses word_tokenize from nltk.tokenize to split file\n",
    "    into words. This function creates tensor of sequential tokens at sequential index. So if the file starts with \"you are\n",
    "     the wizzard, harry.\", and following words have following ids: {'you':124, 'are':412, 'the':26, 'wizzard':25,\n",
    "      ',':432, 'harry':622, '.':11324}, then this tensor will be like [124, 412, 26, 25, 432, 622, 11324, ...]\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 book_filapath,\n",
    "                 dictionary,\n",
    "                 sequence_length,\n",
    "                 transform=None,\n",
    "                 target_transform=None):\n",
    "        \"\"\"\n",
    "        Creates dataset from one file.\n",
    "    Parameters:\n",
    "        book_filapath:\n",
    "            file path to file from which to read, should be .txt file with UTF-8 encoding.\n",
    "        dictionary:\n",
    "            dictionary of id's of each word. Like {'cat':0, 'wizzard':1, ''hermione': 2, ...}\n",
    "        sequence_length:\n",
    "            how many words before and after are used to predict middle word.\n",
    "        transform:\n",
    "            function to be applied on each input in __getitem__ method.\n",
    "        target_transform:\n",
    "            function to be applied on each target in __getitem__ method.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__transform = transform\n",
    "        self.__target_transform = target_transform\n",
    "        self.__sequence_length = sequence_length\n",
    "\n",
    "        try:\n",
    "            with open(book_filapath, 'rt', encoding='UTF-8') as file:\n",
    "                words = word_tokenize(file.read())\n",
    "        except Exception as e:\n",
    "            print(\"There was an error while trying to read words from file: \",\n",
    "                  book_filapath)\n",
    "            print(e)\n",
    "            return\n",
    "        self.tokens = torch.tensor([dictionary[word] for word in words],\n",
    "                                   dtype=torch.long)\n",
    "        self.__length = len(self.tokens) - (2 * sequence_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "    Parameters:\n",
    "        index:\n",
    "            The index of a word. Which is gonna to be predicted.\n",
    "        Return:\n",
    "            A tupple. First element of a tuple is tensor of shape (2*s_l,), where s_l is sequence_length. First part of\n",
    "            tensor are words before word which is to be predicted and second part of tensor are words after word which is\n",
    "            to be predicted. The second element of tuple is token of predicted word.\n",
    "\n",
    "    Suppose sequence_length is 3, and the file passed to constructor, of object of this classed called 'obj', starts with:\n",
    "    \"hogward is the best school for wizzards.\". Suppose those are ids of words used in this sentance:\n",
    "    {'hogward': 123, 'is': 34, 'the': 13645, 'best': 7452, 'school': 15123, 'for': 541, 'wizzards': 231}.\n",
    "\n",
    "    Calling obj[0] should return (tensor([123, 34, 13645, 231, 541, 15123], tensor(7452))\n",
    "    You can't have 'Hogward' as middle word becouse there are no words before. The actual length of the dataset is\n",
    "    nr of words in file (len of list produced by word_tokenize) - 2 * sequence_length.\n",
    "        \"\"\"\n",
    "        index = index + self.__sequence_length\n",
    "        X = torch.cat(\n",
    "            (self.tokens[index - self.__sequence_length:index],\n",
    "             torch.flip(\n",
    "                 self.tokens[index + 1:index + self.__sequence_length + 1],\n",
    "                 (0, ))),\n",
    "            dim=0)\n",
    "        y = self.tokens[index]\n",
    "        if self.__transform is not None:\n",
    "            X = self.__transform(X)\n",
    "        if self.__target_transform is not None:\n",
    "            y = self.__target_transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns length of this dataset.The actual length of the dataset is\n",
    "    nr of words in file (len of list produced by word_tokenize) - 2 * sequence_length.\n",
    "        \"\"\"\n",
    "        return self.__length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#author: Jedrzej Chmiel\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OneItemDataset(Dataset):\n",
    "    \"\"\"This class is used to train encoding and embedding without words_batch. One item of this dataset is just a one\n",
    "    item tensor (shape (,) ). This item is an token representing a word.\"\"\"\n",
    "\n",
    "    def __init__(self, dictionary_length, transform=None):\n",
    "        super().__init__()\n",
    "        self.__transform = transform\n",
    "        self.__length = dictionary_length\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "    Parameters:\n",
    "        index:\n",
    "            An token of w word.\n",
    "    Return:\n",
    "        A one item tensor with this token :).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = torch.tensor(index, dtype=torch.long)\n",
    "        if self.__transform is not None:\n",
    "            x = self.__transform(x)\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.__length\n",
    "\n",
    "#author: Jedrzej Chmiel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk==3.7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from google.colab import drive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAIN_DATA_DIR = 'drive/MyDrive/data_harry_potter'\n",
    "# MAIN_DATA_DIR = 'data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "one_item_dataset = OneItemDataset(len(corpus))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words_batch_datasets = [WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_'+str(i)+'_prepared.txt', corpus.dictionary, sequence_length=6)\n",
    "                        for i in range(1,8)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size=2048) -> float:\n",
    "    \"\"\"\n",
    "Tests passed word batch model using all datasets and returns MSE\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of WordsBatch class to be tested.\n",
    "    datasets:\n",
    "        List of onjects of WordsBatchDataset class. Datasets on which model will be tested.\n",
    "    \"\"\"\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "      loss_function = nn.MSELoss(reduction='sum')\n",
    "      for dataset in datasets:\n",
    "        loader = DataLoader(dataset, batch_size=1024, shuffle=False, drop_last=False)\n",
    "        for X, y in loader:\n",
    "          X = X.to(DEVICE)\n",
    "          y = y.to(DEVICE)\n",
    "          y = model.embedding.to_dense(y)\n",
    "          pred = model(X)\n",
    "          loss = loss_function(pred, y)\n",
    "          mse +=  loss.item()\n",
    "\n",
    "    total_length = sum([len(dataset) for dataset in datasets])\n",
    "    mse = mse / total_length\n",
    "    return mse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None, results:Dict[str, int or float] = None,\n",
    "                    start_epoch:int = 0):\n",
    "    \"\"\"\n",
    "This function trains words_batch model. It uses MSE loss function to do this.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of WordsBatch class.\n",
    "    datasets:\n",
    "        List of all datasets on which model will be trained. First function uses all inputs from first dataset,\n",
    "        then all input from second dataset, ..., then all outputs from last dataset. This is the end of first epoch.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of MSEs before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.MSELoss()\n",
    "    loaders = [DataLoader(dataset, batch_size, shuffle=True) for dataset in datasets]\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "        with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "        with open(saves_dir+'/results.txt', 'wt') as file:\n",
    "            for key, item in results.items():\n",
    "                file.write(key + ': ' + str(item) + '\\n')\n",
    "\n",
    "    end_epoch = start_epoch+epochs\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        for i,loader in enumerate(loaders):\n",
    "            print(f\"Epoch {epoch}/{end_epoch}, dataset {i+1}/7\")\n",
    "            for X, y in tqdm(loader, desc=\"batch: \"):\n",
    "                X = X.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    y = model.embedding.to_dense(y.to(DEVICE))\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if saves_dir is not None:\n",
    "            model.save(saves_dir+'/'+f\"words_batch_epoch_{epoch}.pth\")\n",
    "            if results is not None:\n",
    "                mse = test_words_batch(model, datasets)\n",
    "                print(f\"MSE is: {mse}\")\n",
    "                results[f'mse_after_epoch_{epoch}'] = mse\n",
    "                with open(saves_dir+'/results.pickkle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "                with open(saves_dir+'/results.txt', 'at') as file:\n",
    "                    file.write(f'mse_after_epoch_{epoch} : {mse}\\n')\n",
    "                model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_encoding(model: Embedding, dataset: OneItemDataset, batch_size: int, epochs: int,\n",
    "                   optimizer: optim.Optimizer, saves_dir: str = None, with_training:bool = True) -> List[float]:\n",
    "    \"\"\"\n",
    "This function trains encoding part of Embedding class object. It uses CrossEntropyLoss.\n",
    "Parameters:\n",
    "    model:\n",
    "        Model to be trained. Object of Embedding class.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to train.\n",
    "    batch_size: the batch size to used\n",
    "    optimizer: optimizer to use to update weights and biases.\n",
    "    saves_dir: If not None in this directory function will save the model after each epoch. Files will be named as\n",
    "        words_batch_epoch_1.pth, words_batch_epoch_2.pth, words_batch_epoch_3.pth, ...\n",
    "        If in passed directory already exist file called for example words_batch_epoch_1.pth it will be truncated.\n",
    "        If passed directory does not exist, it will be created.\n",
    "Returns:\n",
    "    List of acurate factor before each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    acurate = 0\n",
    "    acurates = []\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        for y in tqdm(loader, desc=\"batch: \"):\n",
    "            with torch.no_grad():\n",
    "                X = embedding.to_dense(y)\n",
    "            pred = model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "\n",
    "        model.save(saves_dir+'/'+f\"embedding_epoch_{epoch}.pth\")\n",
    "    print(f\"Acurate: {acurate/len(dataset)}\")\n",
    "    acurates.append(acurate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_encoding(model, dataset):\n",
    "    \"\"\"\n",
    "Tests passed embedding model using all datasets and returns acurate factor.\n",
    "Parameters:\n",
    "    model:\n",
    "        Object of Embedding class to be tested.\n",
    "    datasets:\n",
    "        Object of OneItemDataset class. Data on which to test.\n",
    "Return:\n",
    "    Acurate factor (float).\n",
    "    \"\"\"\n",
    "    acurate = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for y in tqdm(dataset):\n",
    "            y = torch.unsqueeze(y, dim=0).to(DEVICE)\n",
    "            X = model.to_dense(y)\n",
    "            pred = model(X)\n",
    "            acurate += torch.count_nonzero(torch.argmin(pred,dim=1)==y).item()\n",
    "    result = acurate/len(dataset)\n",
    "    print(f\"Acurate fraction: {result}\")\n",
    "    return  result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding = Embedding(corpus_size=len(corpus), embedding_size=64, dropout_factor=0.18, sizes=[128, 512])\n",
    "embedding = embedding.to(DEVICE)\n",
    "words_batch = WordsBatch(embedding, hidden_state_size=96, dropout_factor=0.18, sequence_length=6, dense_layer_size=256)\n",
    "words_batch = words_batch.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir_name = MAIN_DATA_DIR+'/models/'+ datetime.now().strftime(\"training_embedding_%d_%m_%Y___%H_%M\")\n",
    "results = words_batch.info()\n",
    "results['mse_initial'] = test_words_batch(words_batch, words_batch_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results, dir_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# words_batch = WordsBatch.load(dir_name+'/words_batch_epoch_6.pth')\n",
    "# words_batch = words_batch.to(DEVICE)\n",
    "# with open(dir_name+'/results.pickle', 'rb') as file:\n",
    "#   results = pickle.load(file)\n",
    "# results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dir_name = MAIN_DATA_DIR+'/models/training_words_batch_09_05_2022___15_58'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer_words_batch = optim.Adam(words_batch.parameters())\n",
    "results['optimizer_type'] = 'Adam'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_words_batch(words_batch, words_batch_datasets, batch_size=16, epochs=15,\n",
    "#               optimizer=optimizer_words_batch, saves_dir=dir_name, results=results,\n",
    "#               start_epoch = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}