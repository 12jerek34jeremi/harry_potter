{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "github_username = '12jerek34jeremi'\n",
    "github_password = getpass.getpass()\n",
    "os.environ['GITHUB_AUTH'] = github_username+':'+github_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://$GITHUB_AUTH@github.com/12jerek34jeremi/harry_potter.git#subdirectory=hpcw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "   Hi, I'm Jędrzej Chmiel and this is description of my latest project, \"Harry Potter Chapter Writter\". It's purpuse is to create a Neural Network that will first learn this magic word and then will write it's own chapter.\n",
    "\n",
    "   How I it is done (short overview):\n",
    "1) Firstly, I bought all seven Harry Potter books (in soft copy) and saved them as pdf. Then I copied all text from each pdf and saved it as .txt file. Then, using regular expression, I removed all unnecessary things from those txt files. By unnecessary things I mean stuff like chapter titles and page numbers. I also took care of hyphynation, underscores and many others, so that word_tokenize function from nltk.tokenizer module can easily tokenize a file.\n",
    "\n",
    "2) Secondly, I use word_tokenize function from tokenize module from tokenize module from nltk package to split text files into words. I gave each unique word an unique id. From this time on I will call those unique id tokens. Token is an integer number and word is a string.\n",
    "\n",
    "3) After that I used trainable embedding (torch.nn.Embedding) to transform tokens to dense vectors. To transform dense vector, which represents some word, back to a token, I used fully-connected layers. From now on I will call dense vector which represents some word an \"embedding vector\". \n",
    "\n",
    "4) Then, I used technique called \"Batch of Words\" to train embedding, so that those embedding vectors really encode usable information and aren' just course of random numbers.\n",
    "\n",
    "5) To write a chapter I'm going to use just LSTM's. Input to one LSTM cell will be an embedding vector, output of one LSTM cell will also be an embedding vector. I will use projection so that hidden state and cell state can be of different dimensions. Then I will use first six books to train this LSTM to predict next word given prevoius nine words. \n",
    "\n",
    "6) Finally I'm going to \"show\" this LSTM cell first six harry potter books and first two chapters of seventh book. I'm going to predict tenth word of third chapter based on first nine word of this chapter, then predict 11-th word of third chapter using 8 'real' words and one prevoisly predicted word, then precith 12-th word using 7 'real' words and two previously predicted words and so on... Finally network is going to predict next word based on only prevoisly predicted word. I will apply this step about INSERT_HERE times to write a chapter. Let's clarify what do I mean by saying \"showing\" first six books. First six books are about INSERT_HERE words. First I predict second word using first word and save cell state and hidden state. Then i predict third word using second real word, saved hidden state, saved cell state and save new hidden and saved hidden state. I will apply this step (predicting next word using prevoius real word, most recent hidden and cell states and saving new hidden and cell state) INSERT_HERE more times. This way I hope LSTM will learn about wizzard word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step One\n",
    "I bought all seven Harry Potter books (in soft copy) and saved them as pdf. Then I copied all text from each pdf and saved it as .txt file. We need a function which will \"prepere\" a book for us. Prepere means removing things like chapter titles and page numbers. I wrote a function which just do so. Here is how this function look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_harry_book(input_directory: str,\n",
    "                       output_directory: str,\n",
    "                       format_mode: int,\n",
    "                       remove_new_lines: bool = True,\n",
    "                       to_lower=True,\n",
    "                       remove_hyphens=True):\n",
    "\n",
    "    patterns = []\n",
    "    patterns.append((re.compile(r'[\"\\”\\“]'), '\"'))\n",
    "    patterns.append((re.compile(r\"[\\'\\’\\`\\´]\"), \"'\"))\n",
    "    patterns.append((re.compile(r\"[\\,\\¸]\"), ','))\n",
    "    patterns.append((re.compile(r'((\\. ?){2,})|…'), ' ... '))\n",
    "\n",
    "    if format_mode == 1:\n",
    "        patterns.append((re.compile(\n",
    "            r'\\nPage [0-9]+ of [0-9]+\\nGet free e-books and video tutorials at www\\.passuneb\\.com\\n'\n",
    "        ), ' '))\n",
    "        patterns.append((re.compile(r'\\n?CHAPTER .*\\n*\\n'), '\\n'))\n",
    "    elif format_mode == 2:\n",
    "        patterns.append((re.compile(\n",
    "            r'\\nC H A P T E R .+\\naTHEaPAGEaSIGNa [0-9]+ aTHEaPAGEaSIGNa\\n[A-Z ]+\\n'\n",
    "        ), ' '))\n",
    "        patterns.append((re.compile(\n",
    "            r'\\n[A-Z \\.\\'\\\"\\,\\-]*\\naTHEaPAGEaSIGNa [0-9]+ aTHEaPAGEaSIGNa\\n'),\n",
    "                         ' '))\n",
    "    elif format_mode == 3:\n",
    "        patterns.append((re.compile(\n",
    "            r\"\\naNUMBERaSIXaSIGNa [0-9]+ aNUMBERaSIXaSIGNa\\nC H A P T E R [A-Z -]+\\n[A-Z ,.’'-]+\\n\"\n",
    "        ), ' '))\n",
    "        patterns.append((re.compile(\n",
    "            r\"\\n[A-Z \\'\\-\\n\\.’]+\\naNUMBERaSIXaSIGNa[\\n ][0-9]+( aNUMBERaSIXaSIGNa)?\\n\"\n",
    "        ), ' '))\n",
    "        patterns.append(\n",
    "            (re.compile(r\"\\nCHAPTER [A-Z -]+\\naNUMBERaSIXaSIGNa [0-9]+\\n\"),\n",
    "             ' '))\n",
    "        patterns.append(\n",
    "            (re.compile(r\"\\nCHAPTER [A-Z -]+\\n[0-9]+ aNUMBERaSIXaSIGNa\\n\"),\n",
    "             ' '))\n",
    "        patterns.append((re.compile(\n",
    "            r\"Get free e-books and video tutorials at www\\.passuneb\\.com\"),\n",
    "                         ' '))\n",
    "    patterns.append((re.compile(r'-\\n'), ''))\n",
    "\n",
    "    if remove_new_lines:\n",
    "        patterns.append((re.compile(r'\\n'), ' '))\n",
    "        patterns.append((re.compile(r'  '), ' '))\n",
    "\n",
    "    patterns.append((re.compile(r'\\. ?\"'), ' . \"'))\n",
    "    patterns.append((re.compile(r\"\\. ?'\"), \" . '\"))\n",
    "\n",
    "    if remove_hyphens:\n",
    "        patterns.append((re.compile(r'[—–\\-/\\\\\\_]'), ' '))\n",
    "    else:\n",
    "        patterns.append((re.compile(r'[/\\\\\\_]'), ' '))\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for input_file in listdir(input_directory):\n",
    "        if input_file[-4:] != '.txt':\n",
    "            print(\n",
    "                \"The directory should contain only the txt files! Can't read file: \",\n",
    "                input_file)\n",
    "            return\n",
    "\n",
    "        with open(input_directory + '/' + input_file, 'rt',\n",
    "                  encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        for pattern, replacement in patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "\n",
    "        if to_lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        with open(output_directory + '/' + input_file[:-4] + '_prepared.txt',\n",
    "                  'wt',\n",
    "                  encoding='utf-8') as file:\n",
    "            file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code is available in <a href=\"https://github.com/12jerek34jeremi/harry_potter.git\">this repository</a> This function is in utils module of hpcw package, which is in that repository in hpcw directory. You can download and import this easily by executing those commands: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://$GITHUB_AUTH@github.com/12jerek34jeremi/harry_potter.git#subdirectory=hpcw\n",
    "from hpcw.utils import prepare_harry_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can just execute below cell to install and import all necessary things at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/12jerek34jeremi/harry_potter.git#subdirectory=hpcw\n",
      "  Cloning https://****@github.com/12jerek34jeremi/harry_potter.git to c:\\users\\jedre\\appdata\\local\\temp\\pip-req-build-dsb0ip6b\n",
      "  Resolved https://****@github.com/12jerek34jeremi/harry_potter.git to commit 71fc8ae1bc53a3971ccdf4820d83819077a73eee\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from hpcw==0.1.8) (1.10.2+cu113)\n",
      "Requirement already satisfied: nltk==3.7 in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from hpcw==0.1.8) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7->hpcw==0.1.8) (4.62.3)\n",
      "Requirement already satisfied: click in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7->hpcw==0.1.8) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7->hpcw==0.1.8) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7->hpcw==0.1.8) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from torch->hpcw==0.1.8) (4.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from click->nltk==3.7->hpcw==0.1.8) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/12jerek34jeremi/harry_potter.git' 'C:\\Users\\jedre\\AppData\\Local\\Temp\\pip-req-build-dsb0ip6b'\n",
      "WARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jedre\\Documents\\.ml_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.7 in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7) (4.62.3)\n",
      "Requirement already satisfied: click in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from nltk==3.7) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jedre\\documents\\.ml_venv\\lib\\site-packages (from click->nltk==3.7) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jedre\\Documents\\.ml_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://$GITHUB_AUTH@github.com/12jerek34jeremi/harry_potter.git#subdirectory=hpcw\n",
    "!pip install nltk==3.7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from hpcw.datasets.words_batch_dataset import WordsBatchDataset\n",
    "from hpcw.datasets.one_item_dataset import OneItemDataset\n",
    "from hpcw.corpus import Corpus\n",
    "from hpcw.models.embedding import Embedding\n",
    "from hpcw.models.words_batch import WordsBatch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from hpcw.utils import count_distance\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function(prepare_harry_book) except for removing chapter titles this function does a little bit more. Two or more dots, next to each other or separated only by space, and … at the end of the word are changed to three dots after a space. (maybe.. --> maybe ... , maybe… --> maybe ..., maybe. . .  -->   maybe ...). This way in the corpus there won't be multiple words representing same thing.\n",
    "\n",
    "   While working on above funtion i discovered a strange behaviour of word_tokenize function from nltk.tokenize. When a sentance is inside quatations marks, then final word, dot and second quatation mark are consider by three different words. That's providing that there are no spaces between dot and second quataion mark. When there is such a space, then dot is consider as part of final word. Dot is also consider as part of word when “ or ” quataion marks are used (not \"). I worked with nltk version 3.7. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk==3.7\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``', 'I', 'have', 'a', 'cat', '.', \"''\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(' \"I have a cat.\" ') # now it is okey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``', 'I', 'have', 'a', 'cat.', '``']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(' \"I have a cat. \" ') # now it is not working as I would like it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“', 'I', 'have', 'a', 'cat.', '“']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('“I have a cat.“')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   I actually missed that at first, so in first version of corpus next to words \"cat\", \"wizzard\" or \"harry\", there were words \"cat.\", \"wizzard.\" and \"harry.\". Not to let that happened I modified prepare_harry_book function a little bit. Now this function first change all both “ ” quatation mark to \" and inserting space before a word, if after the word there is a quatation mark. This way word_tokenize always recognize dot as seperate word, and there aren't as many types of quation marks in the corpus. Above function does a bit more, if you are interested you can read <a href = \"https://github.com/12jerek34jeremi/harry_potter/blob/main/hpcw/hpcw/utils.py\">description </a>which is at the top of function body.\n",
    "    Before going further let's install and install \n",
    "## Step 2\n",
    "   So after removing all unwanted stuff from files we would like to have give each file an unique id, which I will call a token. This is done by this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(directory: str,\n",
    "                      save_file: str = None,\n",
    "                      min_documents: int = 1) -> Dict[str, int]:\n",
    "    \n",
    "    pattern = re.compile(r'^-?[0-9]*[,.]?[0-9]*$')\n",
    "    sets = []\n",
    "    rejected = 0\n",
    "    dictionary = {}\n",
    "    rejected_words = []\n",
    "    for i in range(1, 8):\n",
    "        file_path = directory + '/harry_potter_' + str(i) + '_prepared.txt'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"Could not find file \", file_path)\n",
    "            return None\n",
    "        with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "            sets.append(set(word_tokenize(file.read())))\n",
    "\n",
    "    if len(sets) < min_documents:\n",
    "        print(\n",
    "            f\"There are only {len(sets)} documents in this directory and you require from each word to be in at lest\"\n",
    "            f\" {min_documents} documents!\")\n",
    "        return None\n",
    "    min_documents -= 1\n",
    "    # if word needs to be in at least 3 documents, I need to check if it appears in at least 2 otherdocu ments\n",
    "    i = 0\n",
    "    if min_documents != 0:\n",
    "        i = 3\n",
    "        for i, my_set in enumerate(sets):\n",
    "            to_remove = set()\n",
    "            for word in my_set:\n",
    "                sets_with_the_word = []\n",
    "                n = 0\n",
    "\n",
    "                # start of checking if word is in at least min_documents nr of documents\n",
    "                for other_set in sets[:i] + sets[i + 1:]:\n",
    "                    if word in other_set:\n",
    "                        n += 1\n",
    "                        sets_with_the_word.append(other_set)\n",
    "                        if n == min_documents:\n",
    "                            break\n",
    "                # end of checking if word is in at least min_documents nr of documents\n",
    "\n",
    "                if n < min_documents:\n",
    "                    for other_set in sets_with_the_word:\n",
    "                        other_set.remove(word)\n",
    "                    to_remove.add(word)\n",
    "                    if word.istitle():\n",
    "                        dictionary[word] = 0\n",
    "                    elif pattern.match(word) is not None:\n",
    "                        dictionary[word] = 1\n",
    "                    else:\n",
    "                        dictionary[word] = 2\n",
    "                    rejected += 1\n",
    "                    rejected_words.append(word)\n",
    "            my_set -= to_remove\n",
    "\n",
    "        print(f\"Rejected {rejected} words.\")\n",
    "        print(\"Rejected words:\")\n",
    "        print(rejected_words)\n",
    "\n",
    "    whole_set = set()\n",
    "    for my_set in sets:\n",
    "        whole_set.update(my_set)\n",
    "\n",
    "    del sets\n",
    "\n",
    "    for word in whole_set:\n",
    "        dictionary[word] = i\n",
    "        i += 1\n",
    "\n",
    "    if save_file is not None:\n",
    "        with open(save_file, 'wb') as file:\n",
    "            pickle.dump(dictionary, file)\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function splits text file into words using word_tokenize function and gives each unique word an uinique id. It creates a dictionary which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'wizzard': 1, 'harry': 2, 'music': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplatory_corpus = {\"cat\": 0, 'wizzard': 1, 'harry': 2, 'music': 3}\n",
    "exemplatory_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course real dictionary is much bigger. Having created the dictionary above function then saves it using pickle module in file which filepath is discribed by 'save_file' argument.\n",
    "\n",
    "   The above function has option of including into a corpus only words which are in at least some number of files, but finally I didn't use this. If you are interested you can always read whole function in <a href = \"https://github.com/12jerek34jeremi/harry_potter/blob/main/hpcw/hpcw/utils.py\">description </a>.\n",
    "\n",
    "   You can download dictionary produced by this function from <a href=\"https://drive.google.com/drive/folders/1zQUKZWJ3VnDt-lgd3q--sWG5K_r-DdUP?usp=sharing\">this google drive folder</a> in which there are all data I used or produced while working on this project. If you downloaded aboved folder you can get this dictionary executing this commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re': 0,\n",
       " 'awakened': 1,\n",
       " 'traced': 2,\n",
       " 'dejected': 3,\n",
       " 'deprimo': 4,\n",
       " 'streetlamps': 5,\n",
       " 'nearsighted': 6,\n",
       " 'emporium': 7,\n",
       " 'waters': 8,\n",
       " 'cardboard': 9,\n",
       " 'kept': 10,\n",
       " 'furious': 11,\n",
       " 'sundown': 12,\n",
       " 'punished': 13,\n",
       " 'd.a.': 14,\n",
       " 'journalists': 15,\n",
       " 'suggesting': 16,\n",
       " 'readiness': 17,\n",
       " 'tentacle': 18,\n",
       " 'backfiring': 19,\n",
       " 'hollowed': 20,\n",
       " 'slopingshouldered': 21,\n",
       " 'sacked': 22,\n",
       " 'oozing': 23,\n",
       " 'niche': 24,\n",
       " 'blossom': 25,\n",
       " 'watchers': 26,\n",
       " 'shell': 27,\n",
       " 'stunted': 28,\n",
       " 'finest': 29,\n",
       " 'notorious': 30,\n",
       " 'fairer': 31,\n",
       " 'pumpkin': 32,\n",
       " 'hoisted': 33,\n",
       " 'lightheaded': 34,\n",
       " 'confetti': 35,\n",
       " 'tendrils': 36,\n",
       " 'notoriously': 37,\n",
       " 'providing': 38,\n",
       " 'plums': 39,\n",
       " 'spotty': 40,\n",
       " 'prizing': 41,\n",
       " 'nurtured': 42,\n",
       " 'occupy': 43,\n",
       " 'cried': 44,\n",
       " 'campaigned': 45,\n",
       " 'erected': 46,\n",
       " 'followers': 47,\n",
       " 'quarry': 48,\n",
       " 'slope': 49,\n",
       " 'someday': 50,\n",
       " 'foulest': 51,\n",
       " 'houseelf': 52,\n",
       " 'steering': 53,\n",
       " 'voters': 54,\n",
       " 'dottiness': 55,\n",
       " 'one': 56,\n",
       " 'pleasurable': 57,\n",
       " 'etuunnel': 58,\n",
       " 'holey': 59,\n",
       " 'acknowledges': 60,\n",
       " 'upcoming': 61,\n",
       " 'pursuits': 62,\n",
       " 'blissful': 63,\n",
       " 'collapse': 64,\n",
       " 'doe': 65,\n",
       " 'overlong': 66,\n",
       " 'legilimency': 67,\n",
       " 'inadvertently': 68,\n",
       " 'babysit': 69,\n",
       " 'results': 70,\n",
       " 'chant': 71,\n",
       " 'pomfrey': 72,\n",
       " 'companion': 73,\n",
       " 'ostentatiously': 74,\n",
       " 'bott': 75,\n",
       " 'novelties': 76,\n",
       " 'tumblers': 77,\n",
       " 'slithering': 78,\n",
       " 'restful': 79,\n",
       " 'lightless': 80,\n",
       " 'identified': 81,\n",
       " 'product': 82,\n",
       " 'history': 83,\n",
       " 'grammatica': 84,\n",
       " 'deliverin': 85,\n",
       " 'singularly': 86,\n",
       " 'stowing': 87,\n",
       " 'unexpected': 88,\n",
       " 'matron': 89,\n",
       " 'cavity': 90,\n",
       " 'cue': 91,\n",
       " 'promoted': 92,\n",
       " 'exhausted': 93,\n",
       " 'glugged': 94,\n",
       " 'trumpet': 95,\n",
       " 'saints': 96,\n",
       " 'snare': 97,\n",
       " 'blasts': 98,\n",
       " 'candidate': 99,\n",
       " 'stout': 100,\n",
       " 'froglike': 101,\n",
       " 'assuage': 102,\n",
       " 'understandably': 103,\n",
       " 'numerology': 104,\n",
       " 'misdirect': 105,\n",
       " \"'mazing\": 106,\n",
       " 'rancorous': 107,\n",
       " 'unlike': 108,\n",
       " 'smitten': 109,\n",
       " 'inigo': 110,\n",
       " 'ours': 111,\n",
       " 'interested': 112,\n",
       " 'tasks': 113,\n",
       " 'eaters': 114,\n",
       " 'trickier': 115,\n",
       " 'coding': 116,\n",
       " 'wickedest': 117,\n",
       " 'carved': 118,\n",
       " 'grayish': 119,\n",
       " 'rustled': 120,\n",
       " 'herbione': 121,\n",
       " 'lacquered': 122,\n",
       " 'tentatively': 123,\n",
       " 'pinches': 124,\n",
       " 'weigh': 125,\n",
       " 'sprang': 126,\n",
       " 'belief': 127,\n",
       " 'escapators': 128,\n",
       " 'drained': 129,\n",
       " 'windpipe': 130,\n",
       " 'mistaken': 131,\n",
       " 'pie': 132,\n",
       " 'fowl': 133,\n",
       " 'doubtless': 134,\n",
       " 'robbery': 135,\n",
       " 'cuts': 136,\n",
       " 'map': 137,\n",
       " 'inhaled': 138,\n",
       " 'retorting': 139,\n",
       " 'spit': 140,\n",
       " 'afterthought': 141,\n",
       " 'age': 142,\n",
       " 'maker': 143,\n",
       " 'decipher': 144,\n",
       " 'preserving': 145,\n",
       " 'aroun': 146,\n",
       " 'surprises': 147,\n",
       " 'irresistibly': 148,\n",
       " 'downhill': 149,\n",
       " 'experiment': 150,\n",
       " 'glass': 151,\n",
       " 'lamenting': 152,\n",
       " 'mermen': 153,\n",
       " 'bitty': 154,\n",
       " 'infects': 155,\n",
       " 'terrors': 156,\n",
       " 'protuberance': 157,\n",
       " 'community': 158,\n",
       " 'o.w.l': 159,\n",
       " 'scrap': 160,\n",
       " 'widening': 161,\n",
       " 'pupil': 162,\n",
       " 'furtive': 163,\n",
       " 'underage': 164,\n",
       " 'sincere': 165,\n",
       " 'aaaargh': 166,\n",
       " 'eve': 167,\n",
       " 'outside': 168,\n",
       " 'mutters': 169,\n",
       " 'prick': 170,\n",
       " 'tarantulas': 171,\n",
       " 'breaths': 172,\n",
       " 'raspberries': 173,\n",
       " 'aisles': 174,\n",
       " 'darkening': 175,\n",
       " 'sweatshirt': 176,\n",
       " 'frisbees': 177,\n",
       " 'fortune': 178,\n",
       " 'neighbors': 179,\n",
       " 'dumped': 180,\n",
       " 'woss': 181,\n",
       " 'takeoff': 182,\n",
       " 'revulsion': 183,\n",
       " 'purplish': 184,\n",
       " 'invisibly': 185,\n",
       " 'rides': 186,\n",
       " 'draining': 187,\n",
       " 'goblet': 188,\n",
       " 'yell': 189,\n",
       " 'frank': 190,\n",
       " 'elfric': 191,\n",
       " 'holiday': 192,\n",
       " 'problem': 193,\n",
       " 'happier': 194,\n",
       " 'hang': 195,\n",
       " 'flimsy': 196,\n",
       " 'dummy': 197,\n",
       " 'overrule': 198,\n",
       " 'cowed': 199,\n",
       " 'shacklebolt': 200,\n",
       " 'caughty': 201,\n",
       " 'onionlike': 202,\n",
       " 'bulbs': 203,\n",
       " 'mantra': 204,\n",
       " 'adn': 205,\n",
       " 'assistance': 206,\n",
       " 'soupspoon': 207,\n",
       " 'constrictor': 208,\n",
       " 'safely': 209,\n",
       " 'troupe': 210,\n",
       " 'bellies': 211,\n",
       " 'pompous': 212,\n",
       " 'rampaging': 213,\n",
       " 'potty': 214,\n",
       " 'betide': 215,\n",
       " 'mobile': 216,\n",
       " 'sportingly': 217,\n",
       " 'bole': 218,\n",
       " 'untouched': 219,\n",
       " 'quickening': 220,\n",
       " 'blonde': 221,\n",
       " 'brewing': 222,\n",
       " 'papery': 223,\n",
       " 'pod': 224,\n",
       " 'waterfall': 225,\n",
       " 'macmillan': 226,\n",
       " 'outline': 227,\n",
       " 'entirely': 228,\n",
       " 'opalescent': 229,\n",
       " 'warring': 230,\n",
       " 'tuft': 231,\n",
       " 'skele': 232,\n",
       " 'reignite': 233,\n",
       " 'baths': 234,\n",
       " 'surrounding': 235,\n",
       " 'opened': 236,\n",
       " 'sniffy': 237,\n",
       " 'doily': 238,\n",
       " 'enrage': 239,\n",
       " 'mr': 240,\n",
       " 'dog': 241,\n",
       " 'dreams': 242,\n",
       " 'de': 243,\n",
       " 'foully': 244,\n",
       " 'lounged': 245,\n",
       " 'bezoars': 246,\n",
       " 'notebooks': 247,\n",
       " 'rowle': 248,\n",
       " 'swang': 249,\n",
       " 'adjustment': 250,\n",
       " 'capturing': 251,\n",
       " 'oddity': 252,\n",
       " 'fused': 253,\n",
       " 'wing': 254,\n",
       " 'scuffling': 255,\n",
       " 'jeweled': 256,\n",
       " 'clutches': 257,\n",
       " 'groped': 258,\n",
       " 'vibrated': 259,\n",
       " 'certainty': 260,\n",
       " 'wouldn': 261,\n",
       " 'slumped': 262,\n",
       " 'entangled': 263,\n",
       " 'coldness': 264,\n",
       " 'halftrue': 265,\n",
       " 'irritate': 266,\n",
       " 'conflict': 267,\n",
       " 'cotton': 268,\n",
       " 'desirable': 269,\n",
       " 'dine': 270,\n",
       " 'goggles': 271,\n",
       " 'professorhead': 272,\n",
       " 'guffaws': 273,\n",
       " 'predicted': 274,\n",
       " 'extinguish': 275,\n",
       " 'windmills': 276,\n",
       " 'moldy': 277,\n",
       " 'intimidating': 278,\n",
       " 'inherited': 279,\n",
       " 'practices': 280,\n",
       " 'brawl': 281,\n",
       " 'denser': 282,\n",
       " 'cupids': 283,\n",
       " 'types': 284,\n",
       " 'pitching': 285,\n",
       " 'serpent': 286,\n",
       " 'hurtful': 287,\n",
       " 'unflattering': 288,\n",
       " 'pages': 289,\n",
       " 'sown': 290,\n",
       " 'station': 291,\n",
       " 'rosmerta': 292,\n",
       " 'lancelot': 293,\n",
       " 'ariana': 294,\n",
       " 'genteel': 295,\n",
       " 'aisle': 296,\n",
       " 'communicated': 297,\n",
       " 'past': 298,\n",
       " 'averse': 299,\n",
       " 'uninvestigated': 300,\n",
       " 'loathing': 301,\n",
       " 'materialize': 302,\n",
       " 'emphasized': 303,\n",
       " 'shamrocks': 304,\n",
       " 'diss': 305,\n",
       " 'silliest': 306,\n",
       " 'quid': 307,\n",
       " 'exuberantly': 308,\n",
       " 'cobbled': 309,\n",
       " 'uncommon': 310,\n",
       " 'fixedly': 311,\n",
       " 'negligently': 312,\n",
       " 'principles': 313,\n",
       " 'relinquishing': 314,\n",
       " 'chatter': 315,\n",
       " 'convention': 316,\n",
       " 'precaution': 317,\n",
       " 'reeling': 318,\n",
       " 'excruciatingly': 319,\n",
       " 'rapidly': 320,\n",
       " 'thoughtful': 321,\n",
       " 'chivvied': 322,\n",
       " 'flatteneed': 323,\n",
       " 'cravat': 324,\n",
       " 'sings': 325,\n",
       " 'steadying': 326,\n",
       " 'headache': 327,\n",
       " 'ean': 328,\n",
       " 'writhed': 329,\n",
       " 'offactly': 330,\n",
       " 'felt': 331,\n",
       " 'wardrobe': 332,\n",
       " 'toasted': 333,\n",
       " 'knight': 334,\n",
       " 'wiv': 335,\n",
       " 'key': 336,\n",
       " 'guidance': 337,\n",
       " 'reviewed': 338,\n",
       " 'sleeve': 339,\n",
       " 'stumbling': 340,\n",
       " 'crup': 341,\n",
       " 'waists': 342,\n",
       " \"'s'cuse\": 343,\n",
       " 'declaring': 344,\n",
       " 'believer': 345,\n",
       " 'structure': 346,\n",
       " 'vista': 347,\n",
       " 'somefink': 348,\n",
       " 'misty': 349,\n",
       " 'skinnier': 350,\n",
       " 'purplefaced': 351,\n",
       " 'glowering': 352,\n",
       " 'skeeter': 353,\n",
       " 'mousse': 354,\n",
       " 'herd': 355,\n",
       " 'talking': 356,\n",
       " 'swiping': 357,\n",
       " 'recognizing': 358,\n",
       " 'expectation': 359,\n",
       " 'bewildered': 360,\n",
       " 'assurances': 361,\n",
       " 'blaze': 362,\n",
       " 'encounters': 363,\n",
       " 'reverberating': 364,\n",
       " 'wen': 365,\n",
       " 'unwisely': 366,\n",
       " 'isolated': 367,\n",
       " 'vus': 368,\n",
       " 'dummies': 369,\n",
       " 'smacked': 370,\n",
       " 'endeavors': 371,\n",
       " 'headless': 372,\n",
       " 'bike': 373,\n",
       " 'rider': 374,\n",
       " 'cragg': 375,\n",
       " 'kickoff': 376,\n",
       " 'textbook': 377,\n",
       " 'mission': 378,\n",
       " 'algie': 379,\n",
       " 'turnips': 380,\n",
       " 'deafened': 381,\n",
       " 'veiled': 382,\n",
       " 'both': 383,\n",
       " 'airy': 384,\n",
       " 'greedy': 385,\n",
       " 'ex': 386,\n",
       " 'shifting': 387,\n",
       " 'brown': 388,\n",
       " 'bagpipe': 389,\n",
       " 'susan': 390,\n",
       " 'here': 391,\n",
       " 'incognito': 392,\n",
       " 'bumper': 393,\n",
       " 'cluttering': 394,\n",
       " 'effective': 395,\n",
       " 'wolf': 396,\n",
       " 'jinxing': 397,\n",
       " 'staggerin': 398,\n",
       " 'open': 399,\n",
       " 'corrected': 400,\n",
       " 'standby': 401,\n",
       " 'marked': 402,\n",
       " 'claims': 403,\n",
       " 'independent': 404,\n",
       " 'spoilsport': 405,\n",
       " 'spawned': 406,\n",
       " 'modify': 407,\n",
       " 'floors': 408,\n",
       " 'studded': 409,\n",
       " 'poking': 410,\n",
       " 'review': 411,\n",
       " 'aggression': 412,\n",
       " 'pronouncements': 413,\n",
       " 'graves': 414,\n",
       " 'obscured': 415,\n",
       " 'playstation': 416,\n",
       " 'rockets': 417,\n",
       " 'tobias': 418,\n",
       " 'dad': 419,\n",
       " 'variation': 420,\n",
       " 'burrow': 421,\n",
       " 'shyly': 422,\n",
       " 'undercover': 423,\n",
       " 'journeying': 424,\n",
       " 'creation': 425,\n",
       " 'quentin': 426,\n",
       " 'watched': 427,\n",
       " 'universe': 428,\n",
       " 'uneaten': 429,\n",
       " 'bright': 430,\n",
       " 'yours': 431,\n",
       " 'consoling': 432,\n",
       " 'jamming': 433,\n",
       " 'chained': 434,\n",
       " 'crates': 435,\n",
       " 'proudest': 436,\n",
       " 'signaling': 437,\n",
       " 'accelerating': 438,\n",
       " 'revolt': 439,\n",
       " 'remus': 440,\n",
       " 'suit': 441,\n",
       " 'scraps': 442,\n",
       " 'hotel': 443,\n",
       " 'equals': 444,\n",
       " 'notbe': 445,\n",
       " 'restlessly': 446,\n",
       " 'rhapsodizing': 447,\n",
       " 'bonfires': 448,\n",
       " 'joined': 449,\n",
       " 'factors': 450,\n",
       " 'roan': 451,\n",
       " 'unwound': 452,\n",
       " 'bang': 453,\n",
       " 'windblown': 454,\n",
       " 'indication': 455,\n",
       " 'mark': 456,\n",
       " 'liability': 457,\n",
       " 'smash': 458,\n",
       " 'unjust': 459,\n",
       " 'references': 460,\n",
       " 'marionette': 461,\n",
       " 'unceremoniously': 462,\n",
       " 'vividly': 463,\n",
       " 'robertses': 464,\n",
       " 'tunelessly': 465,\n",
       " 'concern': 466,\n",
       " 'wrought': 467,\n",
       " 'countercharm': 468,\n",
       " 'unplottable': 469,\n",
       " 'bravery': 470,\n",
       " 'lately': 471,\n",
       " 'blurry': 472,\n",
       " 'inflicting': 473,\n",
       " 'condensation': 474,\n",
       " 'readily': 475,\n",
       " 'increased': 476,\n",
       " 'compulsively': 477,\n",
       " 'detests': 478,\n",
       " 'anxiously': 479,\n",
       " 'explore': 480,\n",
       " 'wisest': 481,\n",
       " 'hallucination': 482,\n",
       " 'spinner': 483,\n",
       " 'miss': 484,\n",
       " 'sledges': 485,\n",
       " 'beckoning': 486,\n",
       " 'slush': 487,\n",
       " 'stowed': 488,\n",
       " \"y'are\": 489,\n",
       " 'corks': 490,\n",
       " 'jumpy': 491,\n",
       " 'clockwise': 492,\n",
       " 'firebolts': 493,\n",
       " 'nifflers': 494,\n",
       " 'ever': 495,\n",
       " 'space': 496,\n",
       " 'stuck': 497,\n",
       " 'limitations': 498,\n",
       " 'mostly': 499,\n",
       " 'simmer': 500,\n",
       " 'outmoded': 501,\n",
       " 'unwelcome': 502,\n",
       " 'activate': 503,\n",
       " 'galumphing': 504,\n",
       " 'dictate': 505,\n",
       " 'waist': 506,\n",
       " 'bud': 507,\n",
       " 'pellet': 508,\n",
       " 'snatcher': 509,\n",
       " 'supporters': 510,\n",
       " 'fined': 511,\n",
       " 'undetected': 512,\n",
       " 'parvati': 513,\n",
       " 'pickling': 514,\n",
       " 'treading': 515,\n",
       " 'edam': 516,\n",
       " 'mounds': 517,\n",
       " 'teaches': 518,\n",
       " 'ave': 519,\n",
       " 'stirred': 520,\n",
       " 'tantalizingly': 521,\n",
       " 'haunted': 522,\n",
       " 'endeavoring': 523,\n",
       " 'untucked': 524,\n",
       " 'jars': 525,\n",
       " 'illustrations': 526,\n",
       " 'surest': 527,\n",
       " 'winced': 528,\n",
       " 'upholding': 529,\n",
       " 'snuffbox': 530,\n",
       " 'trod': 531,\n",
       " 'progressed': 532,\n",
       " 'closeted': 533,\n",
       " 'baraabas': 534,\n",
       " 'frenzied': 535,\n",
       " 'humoring': 536,\n",
       " 'messed': 537,\n",
       " 'taught': 538,\n",
       " 'bestial': 539,\n",
       " 'cavelike': 540,\n",
       " 'contradicted': 541,\n",
       " 'tureen': 542,\n",
       " 'children': 543,\n",
       " 'whence': 544,\n",
       " 'really': 545,\n",
       " 'nightmarishly': 546,\n",
       " 'battling': 547,\n",
       " 'plenty': 548,\n",
       " 'atrium': 549,\n",
       " 'sorting': 550,\n",
       " 'copiously': 551,\n",
       " 'dismal': 552,\n",
       " 'sniffing': 553,\n",
       " 'aging': 554,\n",
       " 'problems': 555,\n",
       " 'conversationally': 556,\n",
       " 'potionmaking': 557,\n",
       " 'incantato': 558,\n",
       " 'victoire': 559,\n",
       " 'losing': 560,\n",
       " 'mantelpiece': 561,\n",
       " 'burdens': 562,\n",
       " 'gifts': 563,\n",
       " 'hyacinths': 564,\n",
       " 'watersnakes': 565,\n",
       " 'contender': 566,\n",
       " 'unmanageable': 567,\n",
       " 'bleeder': 568,\n",
       " 'tomatoes': 569,\n",
       " 'grew': 570,\n",
       " 'prejudiced': 571,\n",
       " 'weakly': 572,\n",
       " 'tremble': 573,\n",
       " 'succession': 574,\n",
       " 'manufacturers': 575,\n",
       " 'administer': 576,\n",
       " 'inspect': 577,\n",
       " 'diaphragm': 578,\n",
       " 'cursing': 579,\n",
       " 'slighter': 580,\n",
       " 'burly': 581,\n",
       " 'polyjuice': 582,\n",
       " 'foes': 583,\n",
       " 'creevey': 584,\n",
       " 'spanish': 585,\n",
       " 'hornets': 586,\n",
       " 'burp': 587,\n",
       " 'build': 588,\n",
       " 'bushy': 589,\n",
       " 'department': 590,\n",
       " 'marks': 591,\n",
       " 'millimeter': 592,\n",
       " 'practically': 593,\n",
       " 'cheeky': 594,\n",
       " 'appealingly': 595,\n",
       " 'farmhouse': 596,\n",
       " 'stale': 597,\n",
       " 'rattled': 598,\n",
       " 'enthusiastically': 599,\n",
       " 'brooms': 600,\n",
       " 'untie': 601,\n",
       " 'accepting': 602,\n",
       " 'bathed': 603,\n",
       " 'unbelievable': 604,\n",
       " 'coziness': 605,\n",
       " 'portree': 606,\n",
       " 'dormant': 607,\n",
       " 'choke': 608,\n",
       " 'ties': 609,\n",
       " 'collection': 610,\n",
       " 'turnout': 611,\n",
       " 'safety': 612,\n",
       " 'obliged': 613,\n",
       " 'impressive': 614,\n",
       " 'corruption': 615,\n",
       " 'riotously': 616,\n",
       " 'grilles': 617,\n",
       " 'gryffndor': 618,\n",
       " 'spent': 619,\n",
       " 'backwards': 620,\n",
       " 'binns': 621,\n",
       " 'model': 622,\n",
       " 'images': 623,\n",
       " 'spawns': 624,\n",
       " 'grunt': 625,\n",
       " 'astounded': 626,\n",
       " 'jammed': 627,\n",
       " 'poundage': 628,\n",
       " 'create': 629,\n",
       " 'punched': 630,\n",
       " 'weasley': 631,\n",
       " 'cobwebs': 632,\n",
       " 'theatrically': 633,\n",
       " 'tally': 634,\n",
       " 'bloodshed': 635,\n",
       " 'coped': 636,\n",
       " 'tribe': 637,\n",
       " 'denial': 638,\n",
       " 'regulated': 639,\n",
       " 'fast': 640,\n",
       " 'bedraggled': 641,\n",
       " '49': 642,\n",
       " 'smattering': 643,\n",
       " 'drifted': 644,\n",
       " 'lightheartedness': 645,\n",
       " 'slughorn': 646,\n",
       " 'throw': 647,\n",
       " 'posts': 648,\n",
       " 'wide': 649,\n",
       " 'holds': 650,\n",
       " 'stop': 651,\n",
       " 'corner': 652,\n",
       " 'remainder': 653,\n",
       " 'soothingly': 654,\n",
       " 'quick': 655,\n",
       " 'troublemakers': 656,\n",
       " 'perceived': 657,\n",
       " 'clumsily': 658,\n",
       " 'anchors': 659,\n",
       " 'banner': 660,\n",
       " 'brandnew': 661,\n",
       " 'bearlike': 662,\n",
       " 'successful': 663,\n",
       " 'rue': 664,\n",
       " 'spells': 665,\n",
       " 'giving': 666,\n",
       " 'thethered': 667,\n",
       " 'sprouts': 668,\n",
       " 'skepticism': 669,\n",
       " 'adjusted': 670,\n",
       " 'profile': 671,\n",
       " 'unwonted': 672,\n",
       " 'someone': 673,\n",
       " 'knarl': 674,\n",
       " 'rubber': 675,\n",
       " 'colored': 676,\n",
       " 'townsfolk': 677,\n",
       " 'assistants': 678,\n",
       " \"'ad\": 679,\n",
       " 'bark': 680,\n",
       " 'rinds': 681,\n",
       " 'dancin': 682,\n",
       " 'coating': 683,\n",
       " 'astonished': 684,\n",
       " 'luxembourg': 685,\n",
       " 'fightin': 686,\n",
       " 'trespassed': 687,\n",
       " 'brace': 688,\n",
       " 'overdosed': 689,\n",
       " 'share': 690,\n",
       " 'replaced': 691,\n",
       " 'universal': 692,\n",
       " 'inee': 693,\n",
       " 'iced': 694,\n",
       " 'copse': 695,\n",
       " 'jiggered': 696,\n",
       " 'vying': 697,\n",
       " 'halfpolished': 698,\n",
       " 'asphodel': 699,\n",
       " 'sipping': 700,\n",
       " 'pondered': 701,\n",
       " 'imprinted': 702,\n",
       " 'subtle': 703,\n",
       " 'employees': 704,\n",
       " 'airwaves': 705,\n",
       " 'drip': 706,\n",
       " 'senile': 707,\n",
       " 'stile': 708,\n",
       " 'selfabsorption': 709,\n",
       " 'yule': 710,\n",
       " 'rhubarb': 711,\n",
       " 'prostrate': 712,\n",
       " 'roaring': 713,\n",
       " 'test': 714,\n",
       " 'maisie': 715,\n",
       " 'wair': 716,\n",
       " 'wrestled': 717,\n",
       " 'feasting': 718,\n",
       " 'shocking': 719,\n",
       " 'counseled': 720,\n",
       " 'nancy': 721,\n",
       " 'quavering': 722,\n",
       " 'throttle': 723,\n",
       " 'adding': 724,\n",
       " 'disperse': 725,\n",
       " 'vincent': 726,\n",
       " 'reprieved': 727,\n",
       " 'flay': 728,\n",
       " 'defend': 729,\n",
       " 'profited': 730,\n",
       " 'screamed': 731,\n",
       " 'committing': 732,\n",
       " 'sapling': 733,\n",
       " 'crevices': 734,\n",
       " 'forgive': 735,\n",
       " 'unpopular': 736,\n",
       " 'bunches': 737,\n",
       " 'riddle': 738,\n",
       " 'judging': 739,\n",
       " 'bowed': 740,\n",
       " 'kitten': 741,\n",
       " 'tombstone': 742,\n",
       " \"'cinderella\": 743,\n",
       " 'support': 744,\n",
       " 'benson': 745,\n",
       " 'closing': 746,\n",
       " 'death': 747,\n",
       " 'whiteblond': 748,\n",
       " 'stocking': 749,\n",
       " 'peashooter': 750,\n",
       " 'falls': 751,\n",
       " 'spoonful': 752,\n",
       " 'bustling': 753,\n",
       " \"'right\": 754,\n",
       " 'agonized': 755,\n",
       " 'tempered': 756,\n",
       " 'prongs': 757,\n",
       " 'dim': 758,\n",
       " 'smashed': 759,\n",
       " 'refilling': 760,\n",
       " 'benamed': 761,\n",
       " 'haggis': 762,\n",
       " 'tradition': 763,\n",
       " 'sliver': 764,\n",
       " 'deflecting': 765,\n",
       " 'barty': 766,\n",
       " 'chortling': 767,\n",
       " 'lament': 768,\n",
       " 'hunching': 769,\n",
       " 'trouser': 770,\n",
       " 'dumpy': 771,\n",
       " 'fifty': 772,\n",
       " 'headmistresses': 773,\n",
       " 'upheld': 774,\n",
       " 'cheat': 775,\n",
       " 'reconnaissance': 776,\n",
       " 'lunge': 777,\n",
       " 'corpsefigure': 778,\n",
       " 'slender': 779,\n",
       " 'suicidal': 780,\n",
       " 'fing': 781,\n",
       " 'workshop': 782,\n",
       " 'screeched': 783,\n",
       " 'designed': 784,\n",
       " 'financial': 785,\n",
       " 'blended': 786,\n",
       " 'emmeline': 787,\n",
       " 'hatch': 788,\n",
       " 'lazy': 789,\n",
       " 'rapped': 790,\n",
       " 'imprecations': 791,\n",
       " 'identifying': 792,\n",
       " 'eyeglass': 793,\n",
       " 'a.m.': 794,\n",
       " 'petrify': 795,\n",
       " 'stroked': 796,\n",
       " 'poncho': 797,\n",
       " 'overcoming': 798,\n",
       " 'launch': 799,\n",
       " 'dazzle': 800,\n",
       " 'irresolute': 801,\n",
       " 'hereafter': 802,\n",
       " 'inventions': 803,\n",
       " 'manticore': 804,\n",
       " 'prolonged': 805,\n",
       " 'supposedly': 806,\n",
       " 'incalculable': 807,\n",
       " 'eaten': 808,\n",
       " 'scurryings': 809,\n",
       " 'wound': 810,\n",
       " 'ooooo': 811,\n",
       " 'utmost': 812,\n",
       " 'besides': 813,\n",
       " 'challenging': 814,\n",
       " 'exhibited': 815,\n",
       " 'never': 816,\n",
       " 'undefeated': 817,\n",
       " 'six': 818,\n",
       " 'demanding': 819,\n",
       " 'undressed': 820,\n",
       " 'perfected': 821,\n",
       " 'procedures': 822,\n",
       " 'array': 823,\n",
       " 'defensively': 824,\n",
       " 'massively': 825,\n",
       " 'eat': 826,\n",
       " 'graying': 827,\n",
       " 'destroyed': 828,\n",
       " 'frustratedly': 829,\n",
       " 'backflips': 830,\n",
       " 'superimposed': 831,\n",
       " 'explode': 832,\n",
       " 'unstick': 833,\n",
       " 'complaints': 834,\n",
       " 'cuppa': 835,\n",
       " 'gettin': 836,\n",
       " 'stove': 837,\n",
       " 'bought': 838,\n",
       " 'abrupt': 839,\n",
       " 'gummed': 840,\n",
       " 'jangled': 841,\n",
       " 'dobbs': 842,\n",
       " 'halfexpecting': 843,\n",
       " 'thirty': 844,\n",
       " 'patches': 845,\n",
       " 'booster': 846,\n",
       " 'wireless': 847,\n",
       " 'retort': 848,\n",
       " 'grub': 849,\n",
       " 'proofs': 850,\n",
       " 'vast': 851,\n",
       " 'extrarefined': 852,\n",
       " 'questionnaire': 853,\n",
       " 'rubied': 854,\n",
       " \"'wand\": 855,\n",
       " 'transform': 856,\n",
       " 'intrusion': 857,\n",
       " 'peoples': 858,\n",
       " 'hiya': 859,\n",
       " 'pumpkins': 860,\n",
       " 'cinders': 861,\n",
       " 'mischief': 862,\n",
       " 'fortresslike': 863,\n",
       " 'linking': 864,\n",
       " 'went': 865,\n",
       " 'windows': 866,\n",
       " 'okay': 867,\n",
       " 'newspaper': 868,\n",
       " 'oldest': 869,\n",
       " 'remember': 870,\n",
       " \"c'mere\": 871,\n",
       " 'befoul': 872,\n",
       " 'erratic': 873,\n",
       " 'gryffindorravenclaw': 874,\n",
       " 'wheeling': 875,\n",
       " 'sooty': 876,\n",
       " 'nicer': 877,\n",
       " 'legendary': 878,\n",
       " 'gardener': 879,\n",
       " '53': 880,\n",
       " 'trot': 881,\n",
       " 'vertical': 882,\n",
       " 'decades': 883,\n",
       " 'easing': 884,\n",
       " 'hopkirk': 885,\n",
       " 'slytherins': 886,\n",
       " 'delaney': 887,\n",
       " 'lucius': 888,\n",
       " 'sixteenyear': 889,\n",
       " 'furor': 890,\n",
       " 'crudely': 891,\n",
       " 'rulebreaking': 892,\n",
       " 'lied': 893,\n",
       " 'ld': 894,\n",
       " 'wounds': 895,\n",
       " 'gossip': 896,\n",
       " 'heartily': 897,\n",
       " 'producing': 898,\n",
       " 'refusal': 899,\n",
       " 'ribbons': 900,\n",
       " 'glimpse': 901,\n",
       " 'abused': 902,\n",
       " 'traitorously': 903,\n",
       " 'couple': 904,\n",
       " 'pillow': 905,\n",
       " 'wall': 906,\n",
       " 'rekindled': 907,\n",
       " 'stave': 908,\n",
       " 'fireworks': 909,\n",
       " 'mop': 910,\n",
       " 'breakthrough': 911,\n",
       " 'lolling': 912,\n",
       " 'wrangled': 913,\n",
       " 'porpington': 914,\n",
       " 'surroundings': 915,\n",
       " 'weirdly': 916,\n",
       " 'hesitant': 917,\n",
       " 'luxurious': 918,\n",
       " 'inseparable': 919,\n",
       " 'gabbling': 920,\n",
       " 'custard': 921,\n",
       " 'misting': 922,\n",
       " 'injust': 923,\n",
       " 'whirring': 924,\n",
       " 'b': 925,\n",
       " 'kenneth': 926,\n",
       " 'sardonically': 927,\n",
       " 'crinkled': 928,\n",
       " 'sniggered': 929,\n",
       " 'jerk': 930,\n",
       " 'specializes': 931,\n",
       " 'permetiez': 932,\n",
       " 'square': 933,\n",
       " 'holy': 934,\n",
       " 'ollivander': 935,\n",
       " 'embarrassed': 936,\n",
       " 'sympathize': 937,\n",
       " 'mouth': 938,\n",
       " 'dudders': 939,\n",
       " 'listeners': 940,\n",
       " 'smell': 941,\n",
       " 'disowned': 942,\n",
       " 'squeals': 943,\n",
       " 'arisen': 944,\n",
       " 'resided': 945,\n",
       " 'rifled': 946,\n",
       " 'prickly': 947,\n",
       " 'lighter': 948,\n",
       " 'cube': 949,\n",
       " \"i'd\": 950,\n",
       " 'numbness': 951,\n",
       " 'machine': 952,\n",
       " 'incendio': 953,\n",
       " 'rabbits': 954,\n",
       " 'scufflings': 955,\n",
       " 'kettleburn': 956,\n",
       " 'mell': 957,\n",
       " 'arcs': 958,\n",
       " 'handkerchiefs': 959,\n",
       " 'publishes': 960,\n",
       " 'gonglike': 961,\n",
       " 'foundations': 962,\n",
       " 'mortlake': 963,\n",
       " 'spite': 964,\n",
       " 'absorb': 965,\n",
       " 'unit': 966,\n",
       " 'marriages': 967,\n",
       " 'ado': 968,\n",
       " 'tighten': 969,\n",
       " 'coals': 970,\n",
       " 'furled': 971,\n",
       " 'wafted': 972,\n",
       " 'reseating': 973,\n",
       " 'bearings': 974,\n",
       " 'rictusempra': 975,\n",
       " 'gentlemen': 976,\n",
       " 'gnashing': 977,\n",
       " 'mischievously': 978,\n",
       " 'carrow': 979,\n",
       " \"'something\": 980,\n",
       " 'naturally': 981,\n",
       " 'thus': 982,\n",
       " 'salem': 983,\n",
       " 'purchase': 984,\n",
       " 'humorles': 985,\n",
       " 'waging': 986,\n",
       " 'shrewdly': 987,\n",
       " 'describes': 988,\n",
       " 'sandwich': 989,\n",
       " 'veered': 990,\n",
       " 'wildest': 991,\n",
       " 'straightforward': 992,\n",
       " 'moonstones': 993,\n",
       " 'reluctance': 994,\n",
       " 'progressing': 995,\n",
       " 'middle': 996,\n",
       " 'deprive': 997,\n",
       " 'slurped': 998,\n",
       " 'separately': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_DATA_DIR = 'data' # change this for filepath of the downloaded folder\n",
    "with open(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle', 'rb') as file:\n",
    "    corpus_dictionary = pickle.load(file)\n",
    "corpus_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   In file <a href = \"https://github.com/12jerek34jeremi/harry_potter/blob/main/test_corpus.ipynb\">test_corpus.ipynb</a> i was analyzing this dictionary to make sure that there are no weird words there and that most of words occurs multiple times in books. Good I did it, otherwise I hadn't known that I have words like \"cat.\" in corpus and I hadn't fixed prepere_harry_book function. :D\n",
    "   \n",
    "   While working on this project after a while I came to conlusion that I would like to have an easy way to transform word into a token and token into a word. I wrote this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, dictionary_filepath: str):\n",
    "        try:\n",
    "            with open(dictionary_filepath, 'rb') as file:\n",
    "                self.dictionary = pickle.load(file)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"There was an error while trying to read dictionary from file: \",\n",
    "                dictionary_filepath)\n",
    "            print(e)\n",
    "            return\n",
    "\n",
    "        self.__length = len(self.dictionary)\n",
    "        words = [None for _ in range(self.__length)]\n",
    "        for word, word_id in self.dictionary.items():\n",
    "            words[word_id] = word\n",
    "\n",
    "        if None in words:\n",
    "            print(\"Dictionary saved in file: \", dictionary_filepath,\n",
    "                  \" has a id gap.\")\n",
    "            print(\"There is no word assigned to id: \", words.index(None))\n",
    "            raise Exception(\"Id gap in dictionary.\")\n",
    "            return\n",
    "\n",
    "        self.__words = words\n",
    "\n",
    "    def __getitem__(self, index: str or int):\n",
    "        if isinstance(index, int):\n",
    "            return self.__words[index]\n",
    "        elif isinstance(index, str):\n",
    "            return self.dictionary[index]\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Unsupported index type: \" + str(type(index)) +\n",
    "                \" (in __getitem__ function of Corpus class object)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Returns the number of words in dictionary. (max_id+1)\"\n",
    "        return self.__length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can easily convert tokens to words and words to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token of word \"wizzard\" is  7093\n",
      "Token 7093 is assigned to token \"wizard\".\n"
     ]
    }
   ],
   "source": [
    "my_corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "print('Token of word \"wizzard\" is ', my_corpus['wizard'])\n",
    "print('Token 7093 is assigned to token \"', my_corpus[7093], '\".', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usually, while copying code from repository, I removed all coments, so that cell isn't so long. <a>Here</a> you can view original code (written by me) with comments.\n",
    "\n",
    "## Step Three\n",
    "   Now we would like to have a way to transform token to an embedding vector and to encode the embedding vector (to transform it back to a token). I used trainable embedding (torch.nn.Embedding) with embedding vectors of size (64,) for first transformation. For encoding i used fully-connected layers with three hidden layers. First hidden layer had 256 neuron, second hidden layer had 512 neurons and last hidden layer had 1024 neurins. \n",
    "\n",
    "![title](notebooks_data/embedding_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is implementation of such a network. As usually you can see this code with comments <a href=\"https://github.com/12jerek34jeremi/harry_potter/blob/main/hpcw/hpcw/models/embedding.py\">here</a>. While copyting code from repository I removed comments and methods responsible for loading and saving. I did it so below cell is not so long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Jedrzej Chmiel\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 corpus_size: int,\n",
    "                 embedding_size: int,\n",
    "                 dropout_factor: float,\n",
    "                 sizes=[512, 1024, 2048]):\n",
    "\n",
    "        super().__init__()\n",
    "        self.__embedding = nn.Embedding(corpus_size, embedding_size)\n",
    "\n",
    "        self.__encoding = nn.ModuleList()\n",
    "        for input_dim, output_dim in zip([embedding_size] + sizes[:-1], sizes):\n",
    "            self.__encoding.extend([\n",
    "                nn.Linear(input_dim, output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_factor)\n",
    "            ])\n",
    "        self.__encoding.append(nn.Linear(sizes[-1], corpus_size))\n",
    "\n",
    "        self.corpus_size = corpus_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_factor = dropout_factor\n",
    "        self.sizes = sizes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def get_embedding(self) -> torch.nn.Embedding:\n",
    "        return self.__embedding\n",
    "\n",
    "    def to_dense(self, tokens: torch.Tensor):\n",
    "        return self.__embedding(tokens)\n",
    "\n",
    "    def words_probabilities(self, dense_embedding: torch.Tensor):\n",
    "        result = dense_embedding\n",
    "        for l in self.__encoding:\n",
    "            result = l(result)\n",
    "\n",
    "        return f.softmax(result, dim=-1)\n",
    "\n",
    "    def forward(self, dense_embedding: torch.Tensor):\n",
    "        \n",
    "        result = dense_embedding\n",
    "        for l in self.__encoding:\n",
    "            result = l(result)\n",
    "\n",
    "        return self.log_softmax(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create object of Embedding class you need to specify:\n",
    "1) corpus_size: The size of corpus, how many words there are in dictionary.\n",
    "\n",
    "2) embedding_size: The lenght of a dense vector which will represent a word\n",
    "\n",
    "3) dropout_factor: The dropout factor used in each hidden layer of encoding network.\n",
    "\n",
    "4) sizes: Sizes is list of lengths of consecutive hidden layers in encoding network. Encoding network is fully connected network used for tansforming a dense vector back to the token.\n",
    "\n",
    "In our case (as shown in graph1) corpus_size is 21371, embedding_size is 64, dropout_factor is 0.18 and sizes is [256, 512, 1024]. I tried different options, I used embedding size of 32, 64 and 128. It turned out that 64 is the best options.\n",
    "\n",
    "Class embedding has method to_dense, which you can use to transform token to dense vector and method word_propabilites, which can be used to transfrom embedding vector back to a token. Method word_propabilites returns a tensor, which each element denote the propability that given embedding vector corrensponds to a token of this element's index. So if returened tensor lookes like this:\n",
    "[[0.01, 0.02, 0.93, 0.1, 0.003, ..., 0.01]],\n",
    "then it means that for 93% passed dense_vector represent word of token 2 (because 0.93 is at position [0,2]). Method forward\n",
    "is really similar to method word_propabilites, just instead of propabilities it returns natural logarythms of those propabilites.\n",
    "\n",
    "Below is shown how you can change words to tokens, tokens to embedding vectors and embedding vector back to a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_words:  ['you', \"'re\", 'a', 'wizard', ',', 'harry', '!']\n",
      "my_tokens:  tensor([21322, 10380, 18334,  7093,  5506, 14689,  6547])\n",
      "Shape of dense_vectors:  torch.Size([7, 64])\n",
      "Shape of words_propabilites:  torch.Size([7, 21371])\n",
      "encoded_tokens:  tensor([20033,  1246,  2743, 13680, 20614, 17728,  7476])\n",
      "encoded_word:  ['whimper', 'rats', 'warier', 'icy', 'wandcarriers', 'involvement', 'retreat']\n"
     ]
    }
   ],
   "source": [
    "my_words = ['you', \"'re\", 'a', 'wizard', ',', 'harry', '!']\n",
    "print(\"my_words: \", my_words)\n",
    "my_tokens = torch.tensor([my_corpus[word] for word in my_words], dtype=torch.long)\n",
    "print(\"my_tokens: \", my_tokens)\n",
    "my_embedding = Embedding(len(my_corpus), 64, 0.18, [256, 512, 1024])\n",
    "dense_vectors = my_embedding.to_dense(my_tokens)\n",
    "print(\"Shape of dense_vectors: \", dense_vectors.shape)\n",
    "words_propabilities = my_embedding.words_probabilities(dense_vectors)\n",
    "print(\"Shape of words_propabilites: \", words_propabilities.shape)\n",
    "encoded_tokens = torch.argmax(words_propabilities, dim=1)\n",
    "print(\"encoded_tokens: \", encoded_tokens)\n",
    "encoded_word = [my_corpus[token.item()] for token in encoded_tokens]\n",
    "print(\"encoded_word: \", encoded_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this embedding is not trained at all so predicted tokens and real tokens are completely different.\n",
    "Let's remind terminology used in this notebook:\n",
    "word - A string like 'harry', '.', or 'wizard'.\n",
    "token - A natural number. Each unique word is assigned to unique natural number called a token.\n",
    "embedding_vector - A dense vector representing some word. It is implemented using one dimensional tensor.\n",
    "\n",
    "## Step four\n",
    "At this moment embedding vectors are more a less sequence of random numbers. They doesn't decode any usefull inormation, they aren't meaningfull. To change that I used techinque called \"Batch of Words\". In this technique you are trying to predict a word by previous N words and following N words. Just like you are trying to fill a gap in a sentence. For example given words\n",
    "['Hogward', 'is', 'the'] and words ['school', 'for', 'wizzards'], we want to predict word 'best'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogward is the ---?--- school for wizzards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sketch of \"Batch of Words\" model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](notebooks_data/words_batch_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   As you can see we first change all words to tokens and then all tokens to embedding vectors. Then we use stack of two LSTM's to analyse N prevoius words. First input to LSTM cell is token 1, then token 2 and then token 3. We do the same with N following words, but here we input words in reverse order. First input to LSTM cell is embedding vector of token 7, then embedding vector of token 6 and lastly embedding vector of token 5. Then we create a context vector by concatenating last hidden states of second layer of both LSTM's stack. Then there is a fully connected layer with ReLU activation, and lastly one more fully connected layer, which output is embedding vector. We want this produced embedding vector to be the same what embedding vector of word we are predicting.\n",
    "\n",
    "   Each LSTM cell and first fully-connected layer has dropout applied. This droput is for two things. First of all we want our Batch of Words model to predict word instead of learning sequences by heart. Secondly, which is more importat, this dropout enforce learning of embedding. In embedding there is no dropout applied. Because of that embedding parameters are updated more often than other model parameters. This enforce embedding to encode meaningfull information in embedding vectors. We need to remember that primary purpuse of Batch of Words model is to train embedding, not to predict a word.\n",
    "   \n",
    "   Lastly, while training on given sequence we will not update parameters of embedding ofword that is predicted. (Embedding vector of word, which is predicted, is done with torch.no_grad(). )\n",
    "   \n",
    "   Here is implementation of a Word of Batch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsBatch(nn.Module):  #second version\n",
    "    def __init__(self,\n",
    "                 embedding: Embedding,\n",
    "                 hidden_state_size: int,\n",
    "                 dropout_factor: float,\n",
    "                 sequence_length: int,\n",
    "                 dense_layer_size: int = 1024):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        embedding_size = embedding.embedding_size\n",
    "        self.lstm_before = nn.LSTM(embedding_size,\n",
    "                                   hidden_state_size,\n",
    "                                   2,\n",
    "                                   dropout=dropout_factor,\n",
    "                                   batch_first=True)\n",
    "        self.lstm_after = nn.LSTM(embedding_size,\n",
    "                                  hidden_state_size,\n",
    "                                  2,\n",
    "                                  dropout=dropout_factor,\n",
    "                                  batch_first=True)\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Linear(hidden_state_size * 2, dense_layer_size), nn.ReLU(),\n",
    "            nn.Dropout(dropout_factor),\n",
    "            nn.Linear(dense_layer_size, embedding_size))\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.dropout_factor = dropout_factor\n",
    "        self.dense_layer_size = dense_layer_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        input = self.embedding.to_dense(input)\n",
    "        _, (hiddens_before,\n",
    "            _) = self.lstm_before(input[:, :self.sequence_length, :])\n",
    "        _, (hiddens_after,\n",
    "            _) = self.lstm_after(input[:, self.sequence_length:, :])\n",
    "        return self.tail(\n",
    "            torch.stack([\n",
    "                torch.cat((hiddens_before[1, i], hiddens_after[1, i]), dim=0)\n",
    "                for i in range(batch_size)\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view whole class code <a href=\"https://github.com/12jerek34jeremi/harry_potter/blob/main/hpcw/hpcw/models/words_batch.py\">here</a>, while copying from repository I removed all comments and method responsible for saving and loading from file. To create a Batch of Words model you need to specify following things:\n",
    "1) embedding: The object of Embedding class. It will be used to convert tokens for dense vectors.\n",
    "\n",
    "2) hidden_state_size: The size of hidden_state in both layers of both LSTM's stacks.\n",
    "\n",
    "3) dropout_factor: Dropout factor in LSTM layers and in first fully connected layer.\n",
    "\n",
    "4) sequence_length: How many words before and after will be used to predict the middle word. If sequence_length is 3 then input to this model should be three words before and three words after. (N)\n",
    "\n",
    "  On the above graph hidden_state_size is 96 and sequence_length is 3. Input to a forward function is expected a tensor of shape (N, 2*s_l), where N is batch size and s_l is sequence_length. First part of second axis are words before word which is to be predicted and second part of second axis are words after word which is to be predicted in reversed order.Data type of athe input tensor shoul be \"long\" (intiger numbers).\n",
    "  \n",
    "   Here is an example of how to use WordsBatch class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences:\n",
      "[['hogwart', 'is', 'the', 'best', 'school', 'for', 'wizards'],\n",
      " ['when', 'aunt', 'petunia', 'and', 'dudley', 'had', 'run']]\n",
      "sentences in tokens:\n",
      "[[7418, 9501, 3589, 20972, 6054, 9838, 3089],\n",
      " [15916, 13125, 19518, 13483, 15871, 13211, 20101]]\n",
      "input_sequence:\n",
      "tensor([[ 7418,  9501,  3589,  3089,  9838,  6054],\n",
      "        [15916, 13125, 19518, 20101, 13211, 15871]])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "my_corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "sentences = [['hogwart', 'is', 'the', 'best', 'school', 'for', 'wizards'],\n",
    "            ['when', 'aunt', 'petunia', 'and', 'dudley', 'had', 'run']]\n",
    "print(\"sentences:\")\n",
    "pprint(sentences)\n",
    "sentences = [[my_corpus[word] for word in sentence] for sentence in sentences]\n",
    "print(\"sentences in tokens:\")\n",
    "pprint(sentences)\n",
    "input_sequence = torch.tensor([sentence[0:3] + sentence[6:3:-1] for sentence in sentences], dtype=torch.long)\n",
    "print(\"input_sequence:\")\n",
    "pprint(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedding = Embedding(len(my_corpus), 64, 0.18, [256, 512, 1024])\n",
    "my_words_batch = WordsBatch(my_embedding, hidden_state_size=96,\n",
    "                            dropout_factor=0.18, sequence_length=3, dense_layer_size=256)\n",
    "embedding_vectors = my_words_batch(input_sequence)\n",
    "embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In well trained Batch of Words model, the above two embedding vectors should be similar to embedding vectors of words 'best' and 'and'. Before we start training the model we should create a dataset that will allow us to easily get input sequences. We are going of course  Here is implementation of such dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsBatchDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 book_filapath: str,\n",
    "                 dictionary: dict,\n",
    "                 sequence_length: int,\n",
    "                 transform: callable = None,\n",
    "                 target_transform: callable = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.__transform = transform\n",
    "        self.__target_transform = target_transform\n",
    "        self.__sequence_length = sequence_length\n",
    "\n",
    "        try:\n",
    "            with open(book_filapath, 'rt', encoding='UTF-8') as file:\n",
    "                words = word_tokenize(file.read())\n",
    "        except Exception as e:\n",
    "            print(\"There was an error while trying to read words from file: \",\n",
    "                  book_filapath)\n",
    "            print(e)\n",
    "            return\n",
    "        self.tokens = torch.tensor([dictionary[word] for word in words],\n",
    "                                   dtype=torch.long)\n",
    "        self.__length = len(self.tokens) - (2 * sequence_length)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        index = index + self.__sequence_length\n",
    "        X = torch.cat(\n",
    "            (self.tokens[index - self.__sequence_length:index],\n",
    "             torch.flip(\n",
    "                 self.tokens[index + 1:index + self.__sequence_length + 1],\n",
    "                 (0, ))),\n",
    "            dim=0)\n",
    "        y = self.tokens[index]\n",
    "        if self.__transform is not None:\n",
    "            X = self.__transform(X)\n",
    "        if self.__target_transform is not None:\n",
    "            y = self.__target_transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        return self.__length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again you can view whole class code <a>here</a>, while copying from repository I removed all comments, co cell is not so long. This class take one file, splits it to words using word_tokenize function from nltk.tokenize, then transform all words to tokens using passed dictionary. Calling __getitem__(i) returns tuple. First element of this tuple is input_sequence consisting of tokens of words number i, i+1, i+2, i+3, i+6, i+5m i+4 (if sequence_length is 3). Second element of that tuple is token of word number i+4. Number of word is order of word in file. So calling __getitem__(0) returns tuple, which first element of this tuple is input_sequence consisting of first, second, third, fifth, sixth and seventh words' tokens (if sequence_length is 3). Second element of that tuple is token of fourth word.\n",
    "\n",
    "To create WordsBatchDataset object we need to specify following things:\n",
    "\n",
    "1) book_filapath: file path to file from which to read, should be .txt file with UTF-8 encoding.\n",
    "\n",
    "2) dictionary: dictionary of tokens's of each word. Like {'cat':0, 'wizard':1, ''hermione': 2, ...}\n",
    "\n",
    "3) sequence_length: how many words before and after are used to predict middle word.\n",
    "\n",
    "4) transform: function to be applied on each input in __getitem__ method.\n",
    "\n",
    "5) target_transform: function to be applied on each target in __getitem__ method.\n",
    "\n",
    "Let's take a look of how can you use this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18472,  6494, 12483,  8502,   740, 21322]), tensor(9154), 98483)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus = Corpus(MAIN_DATA_DIR+'/dictionary/second_dictionary_30_04.pickle')\n",
    "words_batch_dataset = WordsBatchDataset(MAIN_DATA_DIR+'/harry_potter_books/prepared_txt/harry_potter_1_prepared.txt',\n",
    "                                       my_corpus.dictionary, 3)\n",
    "(X, y) = words_batch_dataset[21889]\n",
    "(X, y, len(words_batch_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18472,  6494, 12483,  8502,   740, 21322]), tensor(9154), 98483)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for a that can test and train our Batch of Words model. Those are that functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size=2048) -> float:\n",
    "    mse = 0.0\n",
    "    with torch.no_grad():\n",
    "      loss_function = nn.MSELoss(reduction='sum')\n",
    "      for dataset in datasets:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        for X, y in loader:\n",
    "          X = X.to(DEVICE)\n",
    "          y = y.to(DEVICE)\n",
    "          y = model.embedding.to_dense(y)\n",
    "          pred = model(X)\n",
    "          loss = loss_function(pred, y)\n",
    "          mse +=  loss.item()\n",
    "    \n",
    "    total_length = sum([len(dataset) for dataset in datasets])\n",
    "    mse = mse / total_length\n",
    "    return mse\n",
    "\n",
    "def train_words_batch(model:WordsBatch, datasets: List[OneItemDataset], batch_size: int, epochs: int,\n",
    "                    optimizer: optim.Optimizer, saves_dir:str = None, results:Dict[str, int or float] = None,\n",
    "                    start_epoch:int = 0):\n",
    "    \n",
    "    model.train()\n",
    "    loss_function = nn.MSELoss()\n",
    "    loaders = [DataLoader(dataset, batch_size, shuffle=True) for dataset in datasets]\n",
    "    if saves_dir is not None:\n",
    "        if not os.path.exists(saves_dir):\n",
    "            os.makedirs(saves_dir)\n",
    "        with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "        with open(saves_dir+'/results.txt', 'wt') as file:\n",
    "            for key, item in results.items():\n",
    "                file.write(key + ': ' + str(item) + '\\n')\n",
    "\n",
    "    end_epoch = start_epoch+epochs\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        for i,loader in enumerate(loaders):\n",
    "            print(f\"Epoch {epoch}/{end_epoch}, dataset {i+1}/7\")\n",
    "            for X, y in tqdm(loader, desc=\"batch: \"):\n",
    "                X = X.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    y = model.embedding.to_dense(y.to(DEVICE))\n",
    "                pred = model(X)\n",
    "                loss = loss_function(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if saves_dir is not None:\n",
    "            model.save(saves_dir+'/'+f\"words_batch_epoch_{epoch}.pth\")\n",
    "            if results is not None:\n",
    "                mse = test_words_batch(model, datasets)\n",
    "                print(f\"MSE is: {mse}\")\n",
    "                results[f'mse_after_epoch_{epoch}'] = mse\n",
    "                with open(saves_dir+'/results.pickle', 'wb') as file:\n",
    "                    pickle.dump(results, file)\n",
    "                with open(saves_dir+'/results.txt', 'at') as file:\n",
    "                    file.write(f'mse_after_epoch_{epoch} : {mse}\\n')\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Those are function from <a>this notebook</a> that I used to train and test Batch of words. As always, while copying code I removed all comments so cell is not so long. Functtion test_words_batch is just usual function for testing a model. You pass model and datasets and you get Mean Squerd Error. Exception is that instead of passing a dataset you pass list of dataset. Model is tested on each example from each dataset. One dataset represents one book.\n",
    "\n",
    "   Function train_words_batch is a little bit more advanced. It has option to test and (or) save model after each epoch. Not only it tests, but also saves results (consequtives) Mean Squered Error in a folder specified by save_dir argument.\n",
    "   \n",
    "   I trained first epoch with Adam optimizer. Adam has advantage of very quickly finding optimum. It's weakness is that, this is usually a local optimum. So after first epoch I changed optimizer for SGD. I was first increasing learning rate, then decreasing it. Finally I managed to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}